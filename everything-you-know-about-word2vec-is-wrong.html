<!DOCTYPE html><meta charset='UTF-8'><html><head><link rel='alternate' type='application/rss+xml' href='feed.rss' title='A universe of sorts'/><link rel='stylesheet' href='katex/katex.min.css'    integrity='sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X'    crossorigin='anonymous'><!-- The loading of KaTeX is deferred to speed up page rendering --><link rel='stylesheet' href='prism/prism.css'><title> A Universe of Sorts </title><style>@font-face {font-family: 'Blog Mono'; src: url('/static/iosevka-fixed-extended.ttf');}@font-face {font-family: 'Blog Sans'; src: url('/static/Exo2-Regular.ttf');}@font-face {font-family: 'Blog Serif'; src: url('/static/Revans-Regular.ttf');}html { font-size: 100%; }html,body { text-size-adjust: none; -webkit-text-size-adjust: none; -moz-text-size-adjust: none; -ms-text-size-adjust: none; } body { background: linear-gradient(to right, #1565C0 1%, #FFF4DD 1%, #FFF4DD 99%, #E91E63 99%);  color: #000000;  font-family: 'Blog Serif', sans-serif;  font-size: 18px; line-height: 1.4em;  margin-top: 0px;  max-width: 100%; overflow-x: hidden; }
h1, h2, h3, h4, h5 { font-family: 'Blog Sans' }img { display:block; width: 100%; max-width: 800px; height: auto }.container { overflow-x: auto; overflow-y: hidden;  max-width: 80ex; text-align: justify;              margin-top: 0px; height: 100%; min-height: 100%;             padding-left: 50px; padding-right: 50px; background: #FFFFFF;}@media (max-width: 480px) {   .container { margin-left: 1%; margin-right: 1%; }  body { font-size: 30px; }  } @media (max-width: 1024px) {  .container { margin-left: 1%; margin-right: 1%; }  body { font-size: 30px; }}@media (min-width: 1024px) { .container { margin-left: 25%; margin-right: 20%; } }.image { }
a:hover { color: #1a73e8; text-decoration: underline;  }
a { color: #1a73e8; text-decoration: none; }
a:visited { color: #1a73e8; text-decoration: none; }
a:active { color: #1a73e8; text-decoration: none; }

blockquote { margin-left: 0px; margin-right: 0px; } pre, .latexblock, blockquote { border-left-color:#BBB;  border-left-style: solid;      border-left-width: 5px; }pre, blockquote { padding-left: 10px; }
pre { font-family: 'Blog Mono', monospace; font-size: 90%;  }pre {  overflow-x: auto; }.latexblock, blockquote, pre { margin-top: 10px; margin-bottom: 10px; padding-bottom: 5px; padding-top: 5px; background-color: #FFFFFF; }.latexblock { line-height: 1em }
pre, kbd, samp, tt{ font-family:'Blog Mono',monospace; }.inline { white-space: nowrap; background:#efefef; }ul, ol { list-style-position: inside; padding-left: 0; }ul { list-style-type: disclosure-closed; }</style></head><body><div class='container'><h2><a id=everything-you-know-about-word2vec-is-wrong href='#everything-you-know-about-word2vec-is-wrong'> ยง </a><span class='centered'> Everything you know about word2vec is wrong </h2> 
 <span class='centered'>The classic explanation of  <code class='inline'>word2vec</code>, in skip-gram, with negative sampling, 
 <span class='centered'>in the paper and countless blog posts on the internet is as follows:  
 <pre><code>while(1) {
   1. vf = vector of focus word
   2. vc = vector of context word
   3. train such that (vc . vf = 1)
   4. for(0 <= i < negative samples):
           vneg = vector of word *not* in context
           train such that (vf . vneg = 0)
}
</code></pre> 
 <span class='centered'>Indeed, if I google "word2vec skipgram", the results I get are: 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/Word2vec#Training_algorithm><span class='centered'>The wikipedia page which describes the algorithm on a high level </a></li><li><span class='centered'><span class='centered'>  <a href=https://www.tensorflow.org/tutorials/representation/word2vec><span class='centered'>The tensorflow page with the same explanation </a></li><li><span class='centered'><span class='centered'>  <a href=https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b><span class='centered'>The towards data science blog which describes the same algorithm </a></li></ul> 
 <span class='centered'>the list goes on. However,  <i><span class='centered'><i><span class='centered'>every single one of these implementations is wrong </i></i>.  
 <span class='centered'>The original word2vec  <code class='inline'>C</code> implementation does  <i><span class='centered'>not </i> do what's explained above, 
 <span class='centered'>and is  <i><span class='centered'>drastically different </i>. Most serious users of word embeddings, who use 
 <span class='centered'>embeddings generated from  <code class='inline'>word2vec</code> do one of the following things:  
 <ol><li><span class='centered'><span class='centered'> They invoke the original C implementation directly. </li><li><span class='centered'><span class='centered'> They invoke the  <code class='inline'>gensim</code> implementation, which is  <i><span class='centered'>transliterated </i> from the <span class='centered'>C source to the extent that the variables names are the same. </li></ol> 
 <span class='centered'>Indeed, the  <code class='inline'>gensim</code> implementation is the  
 <span class='centered'><i><span class='centered'><i><span class='centered'>only one that I know of which is faithful to the C implementation </i></i>.  
 <h4><a id=the-c-implementation href='#the-c-implementation'> ยง </a><span class='centered'> The C implementation </h4> 
 <span class='centered'>The C implementation in fact maintains  <i><span class='centered'>two vectors for each word </i>, one where 
 <span class='centered'>it appears as a focus word, and one where it appears as a context word. 
 <span class='centered'>(Is this sounding familiar? Indeed, it appears that GloVe actually took this 
 <span class='centered'>idea from  <code class='inline'>word2vec</code>, which has never mentioned this fact!)  
 <span class='centered'>The setup is incredibly well done in the C code:  
 <ul><li><span class='centered'><span class='centered'> An array called  <code class='inline'>syn0</code> holds the vector embedding of a word when it occurs <span class='centered'>as a  <i><span class='centered'>focus word </i>. This is  <i><span class='centered'><i><span class='centered'>random initialized </i></i>. </li></ul> 
 <pre><code>https<span class="token operator">:</span><span class="token comment">//github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>a <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> a <span class="token operator">&lt;</span> vocab_size<span class="token punctuation">;</span> a<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> b <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> b<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    next_random <span class="token operator">=</span> next_random <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">long</span> <span class="token keyword">long</span><span class="token punctuation">)</span><span class="token number">25214903917</span> <span class="token operator">+</span> <span class="token number">11</span><span class="token punctuation">;</span>
    syn0<span class="token punctuation">[</span>a <span class="token operator">*</span> layer1_size <span class="token operator">+</span> b<span class="token punctuation">]</span> <span class="token operator">=</span>
       <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>next_random <span class="token operator">&amp;</span> <span class="token number">0xFFFF</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>real<span class="token punctuation">)</span><span class="token number">65536</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">/</span> layer1_size<span class="token punctuation">;</span>
  <span class="token punctuation">}</span>

</code></pre> 
 <ul><li><span class='centered'><span class='centered'> Another array called  <code class='inline'>syn1neg</code> holds the vector of a word when it occurs <span class='centered'>as a  <i><span class='centered'>context word </i>. This is  <i><span class='centered'><i><span class='centered'>zero initialized </i></i>. </li></ul> 
 <pre><code>https<span class="token operator">:</span><span class="token comment">//github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L365</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span>a <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> a <span class="token operator">&lt;</span> vocab_size<span class="token punctuation">;</span> a<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> b <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> b<span class="token operator">++</span><span class="token punctuation">)</span>
  syn1neg<span class="token punctuation">[</span>a <span class="token operator">*</span> layer1_size <span class="token operator">+</span> b<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
</code></pre> 
 <ul><li><span class='centered'><span class='centered'> During training (skip-gram, negative sampling, though other cases are <span class='centered'>also similar), we first pick a focus word. This is held constant throughout <span class='centered'>the positive and negative sample training. The gradients of the focus vector <span class='centered'>are accumulated in a buffer, and are applied to the focus word <span class='centered'><i><span class='centered'>after it has been affected by both positive and negative samples </i>. </li></ul> 
 <pre><code><span class="token keyword">if</span> <span class="token punctuation">(</span>negative <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>d <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> d <span class="token operator">&lt;</span> negative <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> d<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token comment">// if we are performing negative sampling, in the 1st iteration,</span>
  <span class="token comment">// pick a word from the context and set the dot product target to 1</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>d <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    target <span class="token operator">=</span> word<span class="token punctuation">;</span>
    label <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
    <span class="token comment">// for all other iterations, pick a word randomly and set the dot</span>
    <span class="token comment">//product target to 0</span>
    next_random <span class="token operator">=</span> next_random <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">long</span> <span class="token keyword">long</span><span class="token punctuation">)</span><span class="token number">25214903917</span> <span class="token operator">+</span> <span class="token number">11</span><span class="token punctuation">;</span>
    target <span class="token operator">=</span> table<span class="token punctuation">[</span><span class="token punctuation">(</span>next_random <span class="token operator">>></span> <span class="token number">16</span><span class="token punctuation">)</span> <span class="token operator">%</span> table_size<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>target <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> target <span class="token operator">=</span> next_random <span class="token operator">%</span> <span class="token punctuation">(</span>vocab_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>target <span class="token operator">==</span> word<span class="token punctuation">)</span> <span class="token keyword">continue</span><span class="token punctuation">;</span>
    label <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
  l2 <span class="token operator">=</span> target <span class="token operator">*</span> layer1_size<span class="token punctuation">;</span>
  f <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

  <span class="token comment">// find dot product of original vector with negative sample vector</span>
  <span class="token comment">// store in f</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> f <span class="token operator">+=</span> syn0<span class="token punctuation">[</span>c <span class="token operator">+</span> l1<span class="token punctuation">]</span> <span class="token operator">*</span> syn1neg<span class="token punctuation">[</span>c <span class="token operator">+</span> l2<span class="token punctuation">]</span><span class="token punctuation">;</span>

  <span class="token comment">// set g = sigmoid(f) (roughly, the actual formula is slightly more complex)</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>f <span class="token operator">></span> MAX_EXP<span class="token punctuation">)</span> g <span class="token operator">=</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha<span class="token punctuation">;</span>
  <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>f <span class="token operator">&lt;</span> <span class="token operator">-</span>MAX_EXP<span class="token punctuation">)</span> g <span class="token operator">=</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha<span class="token punctuation">;</span>
  <span class="token keyword">else</span> g <span class="token operator">=</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> expTable<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>f <span class="token operator">+</span> MAX_EXP<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>EXP_TABLE_SIZE <span class="token operator">/</span> MAX_EXP <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha<span class="token punctuation">;</span>

  <span class="token comment">// 1. update the vector syn1neg,</span>
  <span class="token comment">// 2. DO NOT UPDATE syn0</span>
  <span class="token comment">// 3. STORE THE syn0 gradient in a temporary buffer neu1e</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> neu1e<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">+=</span> g <span class="token operator">*</span> syn1neg<span class="token punctuation">[</span>c <span class="token operator">+</span> l2<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> syn1neg<span class="token punctuation">[</span>c <span class="token operator">+</span> l2<span class="token punctuation">]</span> <span class="token operator">+=</span> g <span class="token operator">*</span> syn0<span class="token punctuation">[</span>c <span class="token operator">+</span> l1<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token comment">// Finally, after all samples, update syn1 from neu1e</span>
https<span class="token operator">:</span><span class="token comment">//github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L541</span>
<span class="token comment">// Learn weights input -> hidden</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> syn0<span class="token punctuation">[</span>c <span class="token operator">+</span> l1<span class="token punctuation">]</span> <span class="token operator">+=</span> neu1e<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">;</span>
</code></pre> 
 <h4><a id=why-random-and-zero-initialization href='#why-random-and-zero-initialization'> ยง </a><span class='centered'> Why random and zero initialization? </h4> 
 <span class='centered'>Once again, since none of this actually explained in the original papers 
 <span class='centered'><i><span class='centered'>or on the web </i>, I can only hypothesize.  
 <span class='centered'>My hypothesis is that since the negative samples come from all over the text 
 <span class='centered'>and are not really weighed by frequency, you can wind up picking  <i><span class='centered'>any word </i>, 
 <span class='centered'>and more often than not,  <i><span class='centered'>a word whose vector has not been trained much at all </i>. 
 <span class='centered'>If this vector actually had a value, then it could move the actually important 
 <span class='centered'>focus word randomly.  
 <span class='centered'>The solution is to set all negative samples to zero, so that  
 <span class='centered'><i><span class='centered'>only vectors that have occurred somewhat frequently </i> will affect the representation  
 <span class='centered'>of another vector.  
 <span class='centered'>It's quite ingenious, really, and until this, I'd never really thought of 
 <span class='centered'>how important initialization strategies really are.  
 <h4><a id=why-im-writing-this href='#why-im-writing-this'> ยง </a><span class='centered'> Why I'm writing this </h4> 
 <span class='centered'>I spent two months of my life trying to reproduce  <code class='inline'>word2vec</code>, following 
 <span class='centered'>the paper exactly, reading countless articles, and simply not succeeding. 
 <span class='centered'>I was unable to reach the same scores that  <code class='inline'>word2vec</code> did, and it was not 
 <span class='centered'>for lack of trying.  
 <span class='centered'>I could not have imagined that the paper would have literally fabricated an 
 <span class='centered'>algorithm that doesn't work, while the implementation does something completely 
 <span class='centered'>different.  
 <span class='centered'>Eventually, I decided to read the sources, and spent three whole days convinced 
 <span class='centered'>I was reading the code wrong since literally everything on the internet told me 
 <span class='centered'>otherwise.  
 <span class='centered'>I don't understand why the original paper and the internet contain zero 
 <span class='centered'>explanations of the  <i><span class='centered'>actual </i> mechanism behind  <code class='inline'>word2vec</code>, so I decided to put 
 <span class='centered'>it up myself.  
 <span class='centered'>This also explains GloVe's radical choice of having a separate vector 
 <span class='centered'>for the negative context --- they were just doing what  <code class='inline'>word2vec</code> does, but 
 <span class='centered'>they told people about it  <code class='inline'>:)</code>.  
 <span class='centered'>Is this academic dishonesty? I don't know the answer, and that's a heavy 
 <span class='centered'>question. But I'm frankly incredibly pissed, and this is probably the last 
 <span class='centered'>time I take a machine learning paper's explanation of the algorithm 
 <span class='centered'>seriously again --- from next time, I read the source  <i><span class='centered'>first </i>.  
 <script src="https://utteranc.es/client.js"        repo="bollu/bollu.github.io"        issue-term="pathname"        label="question"        theme="github-light"        crossorigin="anonymous"        async></script></container></body></html>