<!DOCTYPE html><meta charset='UTF-8'><html><head><link rel='alternate' type='application/rss+xml' href='feed.rss' title='A universe of sorts'/><link rel='stylesheet' href='/katex/katex.min.css'    integrity='sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X'    crossorigin='anonymous'><!-- The loading of KaTeX is deferred to speed up page rendering --><link rel='stylesheet' href='/prism/prism.css'><title> A Universe of Sorts </title><style>@font-face {font-family: 'Blog Mono'; src: url('/static/iosevka-fixed-extended.ttf');}@font-face {font-family: 'Blog Sans'; src: url('/static/Exo2-Regular.ttf');}@font-face {font-family: 'Blog Serif'; src: url('/static/Revans-Regular.ttf');}html { font-size: 100%; min-height: 100% }html,body { text-size-adjust: none; -webkit-text-size-adjust: none; -moz-text-size-adjust: none; -ms-text-size-adjust: none; } body { min-height: 100vh;  line-height: 1.5em;  color: #000000;  font-family: serif;  font-size: 12px;  margin-top: 0px;  max-width: 100%; overflow-x: hidden; }
h1, h2, h3, h4, h5 { font-family: 'Blog Mono', monospace; }h1, h2 { column-span: all; }img { display:block; width: 100%; max-width: 800px; height: auto }.container { overflow-x: auto; overflow-y: hidden;              margin-top: 0px; height: 100%; min-height: 100%;             max-width: 100%;              font-size: 18px; line-height: 1.5em;              column-count: auto; column-width: 25em; column-fill: auto;              padding-left: 30px; padding-right: 30px; background: #FFFFFF;}@media (max-width: 480px) {   .container { margin-left: 1%; margin-right: 1%; }  body { font-size: 30px; }  } @media (max-width: 1024px) {  .container { margin-left: 1%; margin-right: 1%; }  body { font-size: 30px; }}@media (min-width: 1024px) { .container { margin-left: 1%; margin-right: 1%; } }.image { }
a:hover { color: #1a73e8; text-decoration: underline;  }
a { color: #1a73e8; text-decoration: none; }
a:visited { color: #1a73e8; text-decoration: none; }
a:active { color: #1a73e8; text-decoration: none; }

blockquote { margin-left: 0px; margin-right: 0px; } pre, .latexblock, blockquote { border-left-color:#BBB;  border-left-style: solid;      border-left-width: 5px; }pre, blockquote { padding-left: 10px; }
pre { font-family: 'Blog Mono', monospace; font-size: 90%;  }pre {  overflow-x: auto; }.latexblock, blockquote, pre { margin-top: 10px; margin-bottom: 10px; padding-bottom: 5px; padding-top: 5px; background-color: #FFFFFF; }.latexblock { line-height: 1em }
pre, kbd, samp, tt{ font-family:'Blog Mono',monospace; }.inline { white-space: nowrap; background:#efefef; }ul, ol { list-style-position: inside; padding-left: 0; }ul { list-style-type: disclosure-closed; }</style></head><body><div class='container'><h2><a id=articles/stuff-i-learnt-in-2019 href='#articles/stuff-i-learnt-in-2019'> ยง </a><span class='centered'> Stuff I learnt in 2019 </h2> 
 <span class='centered'>I write these retrospective blog posts every year since 2017. I tend to post a 
 <span class='centered'>collection of papers, books, and ideas I've stumbled across that year. 
 <span class='centered'>Unfortunately, this year, the paper list will be sparser, since I lost some 
 <span class='centered'>data along the way to the year, and hence I don't have links to everything I 
 <span class='centered'>read. So this is going to be a sparser list, consisting of things that I found 
 <span class='centered'><i><span class='centered'>memorable </i>.  
 <span class='centered'>I also re-organised my website, letting the link die, since keeping it up was 
 <span class='centered'>taking far too many cycles (In particular, CertBot was far too annoying to 
 <span class='centered'>maintain, and the feedback of hugo was also very annoying). I now have a 
 <span class='centered'><i><span class='centered'>single </i> file, the 
 <span class='centered'><a href=https://github.com/bollu/bollu.github.io><span class='centered'><code class='inline'>README.md</code>of the  <code class='inline'>bollu/bollu.github.io</code></a>
 <span class='centered'>repo, 
 <span class='centered'>to which I add notes on things I find interesting. I've bound the  <code class='inline'>i</code> alias 
 <span class='centered'>(for idea) on all my shells everywhere, to open the  <code class='inline'>README.md</code> file, wait 
 <span class='centered'>for me to write to it, run a  <code class='inline'>git commit</code> so I can type out a commit, and 
 <span class='centered'>then push. This has been  <i><span class='centered'>massive </i> for what I manage to write down: I feel 
 <span class='centered'>like I've managed to write down a lot of one-liners / small facts that I've 
 <span class='centered'>picked up which I would not otherwise. I'm attempting to similarly pare down 
 <span class='centered'>other friction-inducing parts of my workflow. Suggestions here would be very 
 <span class='centered'>welcome!  
 <span class='centered'>If there's a theme of the year (insofar as my scattered reading has a 
 <span class='centered'>theme...), it's "lattices and geometry". Geometry in terms of differential 
 <span class='centered'>geometry, topology, and topoi. Lattices in the sense of a bunch of abstract 
 <span class='centered'>interpretation and semantics.  
 <h4><a id=articles/course-work-optimisation-theory-quantum-computation-statistics href='#articles/course-work-optimisation-theory-quantum-computation-statistics'> ยง </a><span class='centered'> Course work: optimisation theory, quantum computation, statistics </h4> 
 <span class='centered'>My course work was less interesting to me this time, due to the fact that I had 
 <span class='centered'>chosen to study some wild stuff earlier on, and now have to take reasonable stuff 
 <span class='centered'>to graduate. However, there were courses that filled in a lot of gaps in my 
 <span class='centered'>self-taught knowledge for me, and the ones I listed were the top ones in that 
 <span class='centered'>regard.  
 <span class='centered'>I wound up reading 
 <span class='centered'><a href=https://web.stanford.edu/~boyd/cvxbook/><span class='centered'>Boyd on optimisation theory </a>, 
 <span class='centered'><a href=http://mmrc.amss.cas.cn/tlb/201702/W020170224608149940643.pdf><span class='centered'>Nielsen and Chuang </a> for quantum computation, 
 <span class='centered'>where I also 
 <span class='centered'><a href=https://github.com/bollu/quantum-course-exercises><span class='centered'>solved a bunch of exercises in Q# </a>
 <span class='centered'>which was very fun and rewarding. I'm beginning to feel that learning quantum 
 <span class='centered'>computation is the right route to grokking things like entanglement and 
 <span class='centered'>superposition, unlike the physics which is first of all much harder due to 
 <span class='centered'>infinite dimensionality, and less accessible since we can't  <i><span class='centered'>program </i> it.  
 <h4><a id=articles/formal-research-work-compilers-formal-verification-programming-languages href='#articles/formal-research-work-compilers-formal-verification-programming-languages'> ยง </a><span class='centered'> Formal research work: Compilers, Formal verification, Programming languages </h4> 
 <span class='centered'>My research work is on the above topics, so I try to stay abreast of what's 
 <span class='centered'>going on in the field. What I've read over the past year on these topics is:  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://popl19.sigplan.org/details/POPL-2019-Research-Papers/71/A-2-I-Abstract-2-Interpretation><span class='centered'><code class='inline'>A^2I</code>: meta-abstract interpretation </a>. <span class='centered'>This paper extends the theory of abstract interpretation to perform abstract <span class='centered'>interpretation on program analyses themselves. I'm not sure how  <i><span class='centered'>useful </i> this <span class='centered'>is going to be, as I still hold on to the belief that AI as a framework is <span class='centered'>too general to allow one to prove complex results. But I am still interested <span class='centered'>in trying to adapt this to some problems I have at hand. Perhaps it's going <span class='centered'>to work. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://dl.acm.org/citation.cfm?id=3341691><span class='centered'>Cubicial Agda </a>. This paper introduces <span class='centered'>cubical type theory and its implementation in Agda. It appears to solve many <span class='centered'>problems that I had struggled with during my formalization of loop <span class='centered'>optimisations: In particular, dealing with Coinductive types in Coq, and that <span class='centered'>of defining quotient types / setoids. Supposedly, cubical Agda makes dealing <span class='centered'>with Coinduction far easier. It allows allows the creation of "real" quotient <span class='centered'>types that respect equality, without having to deal with  <code class='inline'>setoid</code> style <span class='centered'>objects that make for large Gallina terms. I don't fully understand how the <span class='centered'><i><span class='centered'>theory </i> works: In particular, as far as I can tell, the synthetic interval <span class='centered'>type  <code class='inline'>I</code> allows one to only access the start and end points ( <code class='inline'>0</code> and  <code class='inline'>1</code>), <span class='centered'>but not anything in between, so I don't really see how it allows for <span class='centered'>interpolation.  I also don't understand how this allows us to make Univalence <span class='centered'>computable.  I feel I need to practice with this new technology before I'm <span class='centered'>well versed, but it's definitely a paper I'm going to read many, many times <span class='centered'>till I grok it. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/abs/1911.05844><span class='centered'>Naive Cubical type theory </a>. This paper <span class='centered'>promises a way to perform informal reasoning with cubical type theory, the <span class='centered'>way we are able to do so with, say, a polymorphic type theory for lambda <span class='centered'>calculus. The section names such as "how do we think of paths", <span class='centered'>"what can we do with paths", inspire confidence </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://icfp19.sigplan.org/details/icfp-2019-papers/26/Call-By-Need-is-Clairvoyant-Call-By-Value><span class='centered'>Call by need is Clairvoyant call by value </a>. This key insight is to notice that call by need <span class='centered'>is "just" call by value, when we evaluate only those values that are <span class='centered'>eventually forced, and throw away the rest. Thus, if we had an oracle that <span class='centered'>tells us which values are eventually forced, we can convert call by need into <span class='centered'>call by value, relative to this oracle. This cleans up many proofs in the <span class='centered'>literature, and might make it far more intuitive to teach call by need to <span class='centered'>people as well. Slick paper, I personally really enjoyed reading this. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/abs/1803.10228><span class='centered'>Shift/Reset the Penultimate Backpropagator </a><span class='centered'>This paper describes how to implement backprop using delimited continuations. <span class='centered'>Also, supposedly, using staging / building a compiler out of this paradigm <span class='centered'>allows one to write high performance compilers for backprop without having <span class='centered'>to suffer, which is always nice. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www.cs.princeton.edu/~zkincaid/pub/popl19a.pdf><span class='centered'>Closed forms for numerical loops </a><span class='centered'>This paper introduces a new algebra of polynomials with exponentials. It then <span class='centered'>studies the eigenvalues of the matrix that describes the loop, and tries to <span class='centered'>find closed forms in terms of polynomials and exponentials. They choose <span class='centered'>to only work with rationals, but not extensions of rational numbers <span class='centered'>(in terms of field extensions of the rationals). Supposedly, this is easier <span class='centered'>to implement and reason about. Once again, this is a paper I'd like to <span class='centered'>reimplement to understand fully, but the paper is well-done! </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://engineering.purdue.edu/Papers/Sundararajah.pdf><span class='centered'>Composable, sound transformations of Nested recursion and loops </a>. <span class='centered'>This paper attempts to bring ideas from polyhedral compilation <span class='centered'>into working with nested recursion. They create a representation using <span class='centered'>multitape finite automata, using which they provide a representation for <span class='centered'>nested recursion. I was somewhat disappointed that it does not handle <span class='centered'>mutual recursion, since my current understanding is that one can always <span class='centered'>convert nested recursion into a "reasonable" straight line program by <span class='centered'>simply inlining calls and then re-using polyhedral techniques. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://github.com/bollu/blaze/blob/master/notebooks/tutorial.ipynb><span class='centered'>Reimplementation of  <code class='inline'>STOKE</code> at  <code class='inline'>bollu/blaze</code>. </a><span class='centered'>I reimplemented the  <a href=http://stoke.stanford.edu/><span class='centered'>STOKE: stochastic superoptimisation </a><span class='centered'>paper, and much to my delight, it was super-effective at regenerating common <span class='centered'>compiler transformations. I want to use this to generate loop optimisations <span class='centered'>as well, by transforming a polyhedral model of the original program. </li></ul> 
 <h4><a id=articles/internsh-over-the-summer-hacking-on-asterius-haskell-webassembly-compiler href='#articles/internsh-over-the-summer-hacking-on-asterius-haskell-webassembly-compiler'> ยง </a><span class='centered'> Internship at  <a href=http://tweag.io/><span class='centered'>Tweag.io </a> over the summer: Hacking on Asterius (Haskell -> WebAssembly compiler) </h4> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www.tweag.io/posts/2019-09-12-webassembly-internship.html><span class='centered'>Blog post on the progress made by me hacking on Austerius over at Tweag </a></li></ul> 
 <span class='centered'>I really enjoyed my time at Tweag! It was fun, and 
 <span class='centered'><a href=https://github.com/TerrorJack><span class='centered'>Shao Cheng </a>
 <span class='centered'>was a great mentor. I must admit that I was somewhat distracted, by all the new 
 <span class='centered'>and shiny things I was learning thanks to all the cool people there  <code class='inline'>:)</code> In 
 <span class='centered'>particular, I wound up bugging 
 <span class='centered'><a href=http://assert-false.net/arnaud/><span class='centered'>Arnaud Spiwack </a>, 
 <span class='centered'><a href=http://simeon-carstens.com/><span class='centered'>Simeon Carstens </a>, 
 <span class='centered'>and  <a href=https://github.com/mmesch><span class='centered'>Matthias Meschede </a>
 <span class='centered'>quite a bit, about type theory, MCMC sampling, and signal processing of storm 
 <span class='centered'>clouds.  
 <span class='centered'>I wound up reading a decent chunk of GHC source code, and while I can't link 
 <span class='centered'>to specifics here, I understood a lot of the RTS much better than I did before. 
 <span class='centered'>It was an enlightening experience, to say the least, and being paid to hack on 
 <span class='centered'>a GHC backend was a really fun way to spend the summer.  
 <span class='centered'>It also led me to fun discoveries, such as 
 <span class='centered'><a href=https://github.com/ghc/ghc/blob/535a26c90f458801aeb1e941a3f541200d171e8f/compiler/cmm/Debug.hs#L458><span class='centered'>how does one debug debug info? </a> 
 <span class='centered'>I also really loved Paris as a city. My AirBnb host was a charming artist who 
 <span class='centered'>suggest spots for me around the city, which I really appreciated. Getting 
 <span class='centered'>around was disorienting for the first week or so, due to the fact that I could 
 <span class='centered'>not (and still do not) really understand how to decide in which direction to 
 <span class='centered'>walk inside the subways to find a particular line 
 <span class='centered'><i><span class='centered'>going in a particular direction </i>.  
 <span class='centered'>The city has some great spots for quiet work, though! In particular, the 
 <span class='centered'><a href=https://www.anticafe.eu/lieux/louvre-paris-75001/><span class='centered'>Louvre Anticafe </a>
 <span class='centered'>was a really nice place to hang out and grab coffee. The model is great: you 
 <span class='centered'>pay for hours spent at the Anticafe, with coffee and snacks free. They also 
 <span class='centered'>had a discount for students which I gratefully used. 
 <span class='centered'>I bumped into interesting artists, programmers, and students who were open for 
 <span class='centered'>conversation there. I highly recommend hanging out there.  
 <h4><a id=articles/probabilistic-programming-giving-a-talk-at-functionalconf href='#articles/probabilistic-programming-giving-a-talk-at-functionalconf'> ยง </a><span class='centered'> Probabilistic programming & giving a talk at FunctionalConf </h4> 
 <span class='centered'>This was the first talk I'd ever given, and it was on probabilistic programming 
 <span class='centered'>in haskell. In particular, I explained the 
 <span class='centered'><a href=https://github.com/adscib/monad-bayes><span class='centered'><code class='inline'>monad-bayes</code></a> approach of 
 <span class='centered'>doing this, and why this was profitable. 
 <span class='centered'><a href=https://github.com/bollu/functionalconf-2019-slides-probabilistic-programming/blob/master/slides.pdf><span class='centered'>The slides are available here </a>.  
 <span class='centered'>It was a fun experience giving a talk, and I'd like to do more of it, since I 
 <span class='centered'>got a lot out of attempting to explain the ideas to people. I wish I had more 
 <span class='centered'>time, and had a clearer idea of who the audience was. I got quite a bit of 
 <span class='centered'>help from  <a href=https://www.snoyman.com/><span class='centered'>Michael Snoyman </a> to whip the talk into 
 <span class='centered'>shape, which I greatly appreciated.  
 <span class='centered'>The major ideas of probabilistic programming as I described it are 
 <span class='centered'>from Adam Scibior's thesis:  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www.cs.ubc.ca/~ascibior/assets/pdf/thesis.pdf><span class='centered'>Adam Scibior: Formally justified and modular Bayesian inference for probabilistic programs </a></li></ul> 
 <span class='centered'>Along the way, I and others at tweag read the other major papers in the space, 
 <span class='centered'>including:  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1206.3255><span class='centered'>Church, a language for generative models </a>, <span class='centered'>which is nice since it describes it's semantics in terms of sampling. This is <span class='centered'>unlike Adam's thesis, where they define the denotational semantics in terms <span class='centered'>of measure theory, which is then approximated by sampling. </li><li><span class='centered'><span class='centered'>  <a href=https://pdfs.semanticscholar.org/16c5/06c5bb253f7528ddcc80c72673fabf584f32.pdf><span class='centered'>Riemann Manifold Langevin and Hamiltonian Monte Carlo </a><span class='centered'>which describes how to perform Hamiltonian Monte Carlo on the <span class='centered'><i><span class='centered'>information geometry </i> manifold.  So, for example, if we are trying to sample <span class='centered'>from gaussians, we sample from a 2D Riemannian manifold with parameters mean <span class='centered'>and varince, and metric as the  <a href=https://en.wikipedia.org/wiki/Fisher_information_metric><span class='centered'>Fisher information metric </a>. <span class='centered'>This is philosophically the "correct" manifold to sample from, since it <span class='centered'>represents the intrinsic geometry of the space we want to sample from. </li><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1808.08271.pdf><span class='centered'>An elementary introduction to Information geometry by Frank Nielsen </a><span class='centered'>something I stumbled onto as I continued reading about sampling from <span class='centered'>distributions. The above description about the "correct" manifold for <span class='centered'>gaussians comes from this branch of math, but generalises it quite a bit <span class='centered'>further. I've tried to reread it several times as I gradually gained maturity <span class='centered'>in differential geometry. I can't say I understand it just yet, but I hope to <span class='centered'>do so in a couple of months. I need more time for sure to meditate on the <span class='centered'>objects. </li><li><span class='centered'><span class='centered'>  <a href=https://github.com/bollu/shakuni><span class='centered'>Reimplementation of  <code class='inline'>monad-bayes</code></a>. <span class='centered'>This repo holds the original implementation on which the talk is based on. <span class='centered'>I read through the  <code class='inline'>monad-bayes</code> source code, and then re-implemented the <span class='centered'>bits I found interesting. It was a nice exercise, and you can see <span class='centered'>the git history tell a tale of my numerous mis-understandings of MCMC methods, <span class='centered'>till I finally got what the hell was going on. </li></ul> 
 <h4><a id=articles/presburger-arithmetic href='#articles/presburger-arithmetic'> ยง </a><span class='centered'> Presburger Arithmetic </h4> 
 <span class='centered'>Since we use a bunch of  <a href=https://en.wikipedia.org/wiki/Presburger_arithmetic><span class='centered'>presburger arithmetic </a>
 <span class='centered'>for  <a href=http://polyhedral.info/><span class='centered'>polyhedral compilation </a>
 <span class='centered'>which is a large research interest of mine, I've been trying to build a 
 <span class='centered'>"complete" understanding of this space. So this time, I wanted to learn 
 <span class='centered'>how to build good solvers:  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://github.com/bollu/gutenberger><span class='centered'><code class='inline'>bollu/gutenberger</code></a> is a decision <span class='centered'>procedure for Presburger arithmetic that exploits their encoding as finite <span class='centered'>automata. One thing that I was experimenting with was that we only use <span class='centered'>numbers of finite bit-width, so we can explore the entire state space <span class='centered'>of the automata and then perform NFA reduction using <span class='centered'><a href=https://en.wikipedia.org/wiki/DFA_minimization><span class='centered'>DFA minimisation </a>. The <span class='centered'>reference I used for this was the excellent textbook <span class='centered'><a href=https://www7.in.tum.de/~esparza/autoskript.pdf><span class='centered'>Automata theory: An algorithmic approach, Chapter 10 </a></li><li><span class='centered'><span class='centered'>  <a href=http://www.lsv.fr/~haase/documents/ch16.pdf><span class='centered'>The taming of the semi-linear set </a><span class='centered'>This uses a different encoding of presburger sets, which allows them to bound <span class='centered'>a different quantity (the norm) rather than the bitwidth descriptions. This allows <span class='centered'>them to compute  <i><span class='centered'>exponentially </i> better bounds for some operations than <span class='centered'>were known before, which is quite cool. This is a paper I keep trying to <span class='centered'>read and failing due to density. I should really find a week away from civilization <span class='centered'>to just plonk down and meditate upon this. </li></ul> 
 <h4><a id=articles/open-questions-for-which-i-want-answers href='#articles/open-questions-for-which-i-want-answers'> ยง </a><span class='centered'> Open questions for which I want answers </h4> 
 <span class='centered'>I want better references to being able to  <i><span class='centered'>regenerate </i> the inequalities 
 <span class='centered'>description from a given automata which accepts the presburger set automata. 
 <span class='centered'>This will allow one to smoothly switch between the  <i><span class='centered'>geometric </i> description 
 <span class='centered'>and the  <i><span class='centered'>algebraic </i> description. There are some operations that only work 
 <span class='centered'>well on the geometry (such as optimisation), and others that only work well on 
 <span class='centered'>the algebraic description (such as state-space minimisation). I have not found 
 <span class='centered'>any good results for this, only scattered fragments of partial results. 
 <span class='centered'>If nothing else, I would like some kind of intuition for  <i><span class='centered'>why this is hard </i>.  
 <span class='centered'>Having tried my stab at it, the general impression that I have is that the 
 <span class='centered'>space of automata is much larger than the things that can be encoded as 
 <span class='centered'>presburger sets. Indeed, it was shown that automata accept numbers which 
 <span class='centered'>are ultimately periodic.  
 <ul><li><span class='centered'><span class='centered'>  first order logic + "arithmetic with +" + ( <i><span class='centered'>another operation I cannot recall </i>). <span class='centered'>I'm going to fill this in once I re-find the reference. </li></ul> 
 <span class='centered'>But yes, it's known that automata accept a language that's broader than just 
 <span class='centered'>first order logic + "arithmetic with +", which means it's hard to dis-entangle 
 <span class='centered'>the presburger gits from the non-presburger bits of the automata.  
 <h4><a id=articles/prolog href='#articles/prolog'> ยง </a><span class='centered'> Prolog </h4> 
 <span class='centered'>I wanted to get a better understading of how prolog works under the hood, so I began 
 <span class='centered'>re-implementing the  <a href=http://wambook.sourceforge.net/><span class='centered'>WAM: warren abstract machine </a>. 
 <span class='centered'>It's really weird, this is the  <i><span class='centered'>only stable reference </i> I can find to implementing 
 <span class='centered'>high-performance prolog interpreters. I don't really understand how to chase the 
 <span class='centered'>paper-trail in this space, I'd greatly appreciate references. My implementation 
 <span class='centered'>is at  <a href=https://github.com/bollu/warren-cpp/><span class='centered'><code class='inline'>bollu/warren-cpp</code></a>. Unfortunately, 
 <span class='centered'>I had to give up due to a really hard-to-debug bug.  
 <span class='centered'>It's crazy to debug this abstract machine, since the internal representation gets 
 <span class='centered'><i><span class='centered'>super convoluted </i> and hard to track, due to the kind of optimised encoding it 
 <span class='centered'>uses on the heap.  
 <span class='centered'>If anyone has a better/cleaner design for implementing good prologs, I'd love 
 <span class='centered'>to know.  
 <span class='centered'>Another fun paper I found in this space thanks to Edward Kmett was 
 <span class='centered'><a href=http://www.drdobbs.com/architecture-and-design/the-rete-matching-algorithm/184405218><span class='centered'>the Rete matching algorithm </a>, 
 <span class='centered'>which allows one to declare many many pattern matches, which are then "fused" 
 <span class='centered'>together into an optimal matcher that tries to reuse work across failed 
 <span class='centered'>matchers.  
 <h4><a id=articles/general-relativity href='#articles/general-relativity'> ยง </a><span class='centered'> General Relativity </h4> 
 <span class='centered'>This was on my "list of things I want to understand before I die", so I wound 
 <span class='centered'>up taking up an Independent Study in university, which basically means that 
 <span class='centered'>I study something on my own, and visit a professor once every couple weeks, 
 <span class='centered'>and am graded at the end of the term. For GR, I wound up referencing a wide 
 <span class='centered'>variety of sources, as well as a bunch of pure math diffgeo books. I've read 
 <span class='centered'>everything referenced to various levels. I feel I did take away the core 
 <span class='centered'>ideas of differential and Riemannian geometry. I'm much less sure I've grokked 
 <span class='centered'>general relativity, but I can at least read the equations and I know all the 
 <span class='centered'>terms, so that's something.  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://theoreticalminimum.com/courses/general-relativity/2012/fall><span class='centered'>The theoretical minimum by Leonard Susskind </a>. <span class='centered'>The lectures are breezy in style, building up the minimal theory (and no proofs) <span class='centered'>for the math, and a bunch of lectures spent analysing the physics. While I wish <span class='centered'>it were a little more proof heavy, it was a really great reference to learn the <span class='centered'>basic theory! I definitely recommend following this and then reading other <span class='centered'>books to fill in the gaps. </li><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/Gravitation_(book><span class='centered'>Gravitation by Misner Thorne and Wheeler </a>) <span class='centered'>This is an imposing book. I first read through the entire thing (Well, the parts I thought I needed), <span class='centered'>to be able to get a vague sense of what they're going for. They're rigorous in <span class='centered'>a very curious way: It has a bunch of great  <i><span class='centered'>physics </i> perspectives of looking <span class='centered'>at things, and that was invaluable to me. Their view of forms as "slot machines" <span class='centered'>is also fun. In general, I found myself repeatedly consulting this book for <span class='centered'>the "true physical" meaning of a thing, such as curvature, parallel transport, <span class='centered'>the equation of a geodesic, and whatnot. </li><li><span class='centered'><span class='centered'>  <a href=http://www2.ing.unipi.it/griff/files/dC.pdf><span class='centered'>Differential Geometry of Curves and Surfaces by do Carmo </a><span class='centered'>This is the best book to intro differential geometry I found. It throws away <span class='centered'>all of the high powered definitions that "modern" treatments offer, and <span class='centered'>starts from the ground up, building up the theory in 2D and 3D. This is amazing, <span class='centered'>since it gives you small, computable examples for things like <span class='centered'>"the Jacobian represents how tangents on a surface are transformed locally". </li><li><span class='centered'><span class='centered'>  <a href=https://www.youtube.com/watch?v=pXGTevGJ01o&list=PLDfPUNusx1EoVnrQcCRishydtNBYU6A0c><span class='centered'>Symplectic geometry & classical mechanics by Tobias Osborne </a><span class='centered'>This lecture series was great, since it re-did a lot of the math I'd seen <span class='centered'>in a more physicist style, especially around vector fields, flows, and <span class='centered'>Lie brackets. Unfortunately for me, I never even  <i><span class='centered'>got </i> to the classical <span class='centered'>mechanics part by the time the semester ended. I began <span class='centered'><a href=https://github.com/bollu/notes/blob/master/diffgeo/main.pdf><span class='centered'>taking down notes in my repo </a>, <span class='centered'>which I plan to complete. </li><li><span class='centered'><span class='centered'>  <a href=https://sites.math.washington.edu/~lee/Books/ISM/><span class='centered'>Introduction to Smooth manifolds: John Lee </a><span class='centered'>This was a very well written  <i><span class='centered'>mathematical </i> introduction to differential geometry. <span class='centered'>So it gets to the physically important bits (metrics, covariant derivatives) <span class='centered'>far later, so I mostly used it as a reference for problems and more rigour. </li><li><span class='centered'><span class='centered'>  <a href=http://hermes.ffn.ub.es/luisnavarro/nuevo_maletin/Einstein_GRelativity_1916.pdf><span class='centered'>Einstein's original paper introducing GR, translated </a><span class='centered'>finally made it click as to  <i><span class='centered'>why </i><span class='centered'>he wanted to use tensor equations: tensor equations of the form  <code class='inline'>T = 0</code> are <span class='centered'>invariant in  <i><span class='centered'>any coordinate system </i>, since on change of coordinates,  <code class='inline'>T</code><span class='centered'>changes by a multiplicative factor! It's a small thing in hindsight, but it <span class='centered'>was nice to see it explicitly spelled out, since as I understand, no one <span class='centered'>among the physicists knew tensor calculus at the time, so he had to introduce <span class='centered'>all of it. </li></ul> 
 <h4><a id=articles/discrete-differential-geometry href='#articles/discrete-differential-geometry'> ยง </a><span class='centered'> Discrete differential geometry </h4> 
 <span class='centered'>I can't recall how I ran across this: I think it was because I was trying to 
 <span class='centered'>get a better understanding of Cohomology, which led me to Google for 
 <span class='centered'>"computational differential geometry", that finally led me to Discrete 
 <span class='centered'>differential geometry.  
 <span class='centered'>It's a really nice collection of theories that show us how to discretize 
 <span class='centered'>differential geometry in low dimensions, leading to rich intuitions and 
 <span class='centered'>a myriad of applications for computer graphics.  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf><span class='centered'>The textbook by Kennan Crane on the topic </a><span class='centered'>which I read over the summer when I was stuck (more often than I'd like) in <span class='centered'>the Paris metro. The book is very accessible, and requires just some <span class='centered'>imagination to grok. Discretizing differential geometry leads to most things <span class='centered'>being linear algebra, which means one can calculate things on paper easily. <span class='centered'>That's such a blessing. </li><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1204.6216><span class='centered'>Geodesics in Heat </a><span class='centered'>explores a really nice way to discover geodesics by simulating the heat <span class='centered'>equation for a short time. The intuition is that we should think of the heat <span class='centered'>equation as describing the evolution of particles that are performing random <span class='centered'>walks. Now, if we simulate this system for a short while and then look at the <span class='centered'>distribution, particles that reach a particular location on the graph <span class='centered'><i><span class='centered'>must have taken the shortest path </i>, since any longer path would not have <span class='centered'>allowed particles to reach there. Thus, the distribution of particles at <span class='centered'>time  <code class='inline'>dt</code> does truly represent distances from a given point.  The paper <span class='centered'>explores this analogy to find accurate geodesics on complex computational <span class='centered'>grids. This is aided by the use of differential geometry, appropriately <span class='centered'>discretized. </li><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1805.09170.pdf><span class='centered'>The vector heat method </a><span class='centered'>explores computing the parallel transport of a vector across a discrete <span class='centered'>manifold efficiently, borrowing techniques from the 'Geodesics in Heat' <span class='centered'>paper. </li><li><span class='centered'><span class='centered'>  <a href=https://www.cs.cmu.edu/~kmcrane/Projects/LieGroupIntegrators/paper.pdf><span class='centered'>Another paper by Kennan Crane: Lie group integrators for animation and control of vehicles </a><span class='centered'>This paper describes a general recipe to tailor-make integrators for a system <span class='centered'>of constraints, by directly integrating over the lie group of the <span class='centered'>configuration space.  This leads to much more stable integrators. I have some <span class='centered'>misguided hope that we can perhaps adapt these techniques to build better FRP <span class='centered'>(functional reactive programming) systems, but I need to meditate on this a <span class='centered'>lot more to say anything definitively. </li></ul> 
 <h4><a id=articles/synthetic-differential-geometry href='#articles/synthetic-differential-geometry'> ยง </a><span class='centered'> Synthetic differential geometry </h4> 
 <span class='centered'>It was  <a href=http://assert-false.net/arnaud/><span class='centered'>Arnaud Spiwack </a>
 <span class='centered'>who pointed me to this. It's a nice axiomatic 
 <span class='centered'>system of differential geometry, where we can use physicist style proofs of 
 <span class='centered'>"taking stuff upto order  <code class='inline'>dx</code>", and having everything work upto mathematical 
 <span class='centered'>rigor.  
 <span class='centered'>The TL;DR is that we want to add a new number called  <code class='inline'>dx</code> into the reals, 
 <span class='centered'>such that  <code class='inline'>dx^2 = 0</code>. But if such a number did exist, then clearly  <code class='inline'>dx = 0</code>. 
 <span class='centered'>However, the punchline is that to prove that  <code class='inline'>dx^2 = 0 => dx = 0</code> requires 
 <span class='centered'>the use of contradiction!  
 <span class='centered'>So, if we banish the law of excluded middle (and therefore no longer use 
 <span class='centered'>proof by contradiction), we are able to postulate the existence of a new 
 <span class='centered'>element  <code class='inline'>dx</code>, which obeys  <code class='inline'>dx^2 = 0</code>. Using this, we can build up the 
 <span class='centered'>whole theory of differential geometry in a pleasing way, without having to 
 <span class='centered'>go through the horror that is real analysis. (I am being hyperbolic, but really, 
 <span class='centered'>real analytic proofs are not pleasant).  
 <span class='centered'><a href=https://www.github.com/bollu/diffgeo><span class='centered'>I began formalizing this in Coq and got a formalism going:  <code class='inline'>bollu/diffgeo</code></a>.  
 <span class='centered'>Once I was done with that, I realised I don't know how to exhibit  <i><span class='centered'>models </i> of 
 <span class='centered'>the damn thing! So, reading up on that made me realise that I need around 8 
 <span class='centered'>chapters worth of a grad level textbook (the aptly named 
 <span class='centered'><a href=https://link.springer.com/book/10.1007/978-1-4757-4143-8><span class='centered'>Models of Smooth Infinitesimal Analysis </a>).  
 <span class='centered'>I was disheartened, so I  <a href=https://mathoverflow.net/questions/346385/constructing-computable-synthetic-differential-geometry><span class='centered'>asked on  <code class='inline'>MathOverflow</code></a>
 <span class='centered'>(also my first ever question there), where I learnt about tangent categories and 
 <span class='centered'>differential lambda calculus. Unfortunately, I don't have the bandwidth to read 
 <span class='centered'>another 150-page tome, so this has languished.  
 <h4><a id=articles/optimisation-on-manifolds href='#articles/optimisation-on-manifolds'> ยง </a><span class='centered'> Optimisation on Manifolds </h4> 
 <span class='centered'>I began reading 
 <span class='centered'><a href=http://www.eeci-institute.eu/GSC2011/Photos-EECI/EECI-GSC-2011-M5/book_AMS.pdf><span class='centered'>Absil: Optimisation on matrix manifolds </a>
 <span class='centered'>which describes how to perform optimisation / gradient descent on 
 <span class='centered'><i><span class='centered'>arbitrary Riemannian manifolds </i>, as well as closed forms for well-known 
 <span class='centered'>manifolds. The exposition in this book is really good, since it picks a 
 <span class='centered'>concrete manifold and churns out all the basic properties of it manually. The 
 <span class='centered'>only problem I had with the books was that there were quite a few gaps (?) in 
 <span class='centered'>the proofs -- perhaps I missed a bunch.  
 <span class='centered'>This led me to learn Lie theory to some degree, since that was the natural 
 <span class='centered'>setting for many of the proofs. I finally saw  <i><span class='centered'>why </i> anyone gives a shit about 
 <span class='centered'>the tangent space at the identity: because it's  <i><span class='centered'>easier to compute! </i> For a 
 <span class='centered'>flavour of this, 
 <span class='centered'><a href=https://math.stackexchange.com/questions/3389983/explicit-description-of-tangent-spaces-of-on><span class='centered'>consider this question on  <code class='inline'>math.se</code> by me that asks about computing tangent spaces of  <span class='latexinline'><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span></a>.  
 <h4><a id=articles/aircs-workshop href='#articles/aircs-workshop'> ยง </a><span class='centered'> AIRCS workshop </h4> 
 <span class='centered'>I attended the 
 <span class='centered'><a href=https://intelligence.org/ai-risk-for-computer-scientists/><span class='centered'>AI risk for computer scientists </a>
 <span class='centered'>workshop hosted by 
 <span class='centered'><a href=https://intelligence.org/><span class='centered'>MIRI (Machine intelligence research institute) </a> in 
 <span class='centered'>December. Here, a bunch of people were housed at a bed & breakfast for a 
 <span class='centered'>week, and we discussed AI risk, why it's potentially the most important thing 
 <span class='centered'>to work on, and anything our hearts desired, really. I came away with new 
 <span class='centered'>branches of math I wanted to read, a better appreciation of the AI risk 
 <span class='centered'>community and a sense of what their "risk timelines" were, and some 
 <span class='centered'>explanations about sheaves and number theory that I was sorely lacking. All in 
 <span class='centered'>all, it was a great time, and I'd love to go back.  
 <h4><a id=articles/p-adic-numbers href='#articles/p-adic-numbers'> ยง </a><span class='centered'> P-adic numbers </h4> 
 <span class='centered'>While I was on a particularly rough flight back from the USA to India when 
 <span class='centered'>coming back from the AIRCS workshop, I began to read the textbook 
 <span class='centered'><a href=https://www.springer.com/gp/book/9783540629115><span class='centered'>Introduction to p-adic numbers by Fernando Gouvea </a>, 
 <span class='centered'>which fascinated me, so I then 
 <span class='centered'><a href=http://bollu.github.io/#a-motivation-for-p-adic-analysis><span class='centered'>wrote up the cool parts introduced in the first two chapters as a blog post </a>. 
 <span class='centered'>I wish to learn more about the p-adics and p-adic analysis, since they 
 <span class='centered'>seem to be deep objects in number theory.  
 <span class='centered'>In particular, a question that I thought might have a somewhat trivial answer 
 <span class='centered'>( <a href=https://math.stackexchange.com/questions/3482489/why-does-the-p-adic-norm-use-base-p><span class='centered'>why do the p-adics use base p in defining norm </a>) 
 <span class='centered'>turned out to have answers that were quite deep, which was something 
 <span class='centered'>unexpected and joyful!  
 <h4><a id=articles/topology-of-functional-programs href='#articles/topology-of-functional-programs'> ยง </a><span class='centered'> Topology of functional programs </h4> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://cs.ioc.ee/ewscs/2012/escardo/slides.pdf><span class='centered'>Slides by Martฤฑn Escardo </a></li><li><span class='centered'><span class='centered'>  <a href=https://pdf.sciencedirectassets.com/272990/1-s2.0-S1571066104X00177/1-s2.0-S1571066104051357/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGQAb828p1io4csznEej60j0PwJteuXf7OoHLSCDhkUTAiEA9ITs1JrUEOE%2Ft%2Fl5TI9ZkNLUfBIx42IZ%2FoAqQpdX4twq2AII6%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDBQOxJ3s3HCbVxSheCqsAt65yorZMMtIhLF7ML5sJQ9S5wZxBayKDrRkUKOjSzXxtQWebXs70FXhhpToXKvJoDrLgsqDzdF%2FAshZY%2FkDUep6KILxKnYxCBrBINhjFqxDlRZH0s1Y991RgCyNNnwmFI%2BrH70lSrV0OxQ9Z5WdXXPDkTLXv8512Xz%2BJn5DqEqdqD49FLbOuHl6PRM0TnYyNJkLBXNVwt75kkGkaTfdgmgiqh7YpcXcHlbqI2oqNcaxFDewXwKDCC7qWD6ECclLgszoeOXOtRI91nvNac8%2BLV4bXkKLXpd99H94N2vXPUPz99p6oqfdY9ixtcfI9POFX8agUilYjXKVhAWk4FSzzzMqbtZLBfkCZT4ffDTxRgL52yD%2FmL5E0Pe4mczVlUoB5DKoB8Lkitrt0BumQzDr4ffvBTrQAjVRuzG5V0CC%2Fd1t%2BUMPkrywaYytbrXCZ%2BkDo0xDBqsljY8DaGIiFINr8BEEpT7UX42GRhcDzpnOnztdAOTea3qZ3SmXJwgEoh0aiz%2B87MmsC57s0Q%2F%2B%2FDDvHBY3zLCrz7rdewXOgk6VxI9d5mhG3Du1dwPRbgOe798S2waDCD8LQA3rw7w5wNGa9Uv3xtNVH%2BHw%2FXcQ6OiubO4GL9mK8U5g7TVPh1hLB26XBQooKJ564VGf4J9VqWxjlx3NicVhqnFlGevNJNKyVLiyRsRCyQGMV59%2BXqUwdEMQZYWLbfUwELNz1NKfWumvu9BXC5jjsJgNx%2FRERSb7hqT1svMJU91o%2FHtatGAnPvVjYaNthha9O9jm%2BG9nw1vMsdyJ0asI5w5SrlsEyb5C7Vk7aLBcHAEi3XPRhivY1Q4hZAN0xY9VfEZrF%2FoM9HCGxr5cYs%2BP9w%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20191221T113658Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6QRXTPOC%2F20191221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8a26656e8f8c8772afdc2ae6caa9b583fd59a8ee6303d9770d25b4a3d8a6291f&hash=42518d3e5cc1e77ed961c359d0ea8f59bd582d1e5f13d69c3eeb2563a6c82abd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1571066104051357&tid=spdf-584e8188-4806-44bb-a118-9b63a53caaff&sid=f53691052392834d3e18a62484e2909639a9gxrqb&type=client><span class='centered'>Synthetic topology of data types and classical spaces </a></li></ul> 
 <span class='centered'>Both of these describe a general method to transfer all topological ideas as 
 <span class='centered'>statements about  <i><span class='centered'>computability </i> in a way that's far more natural (at least for 
 <span class='centered'>me, as a computer scientist). The notion of what a continuous function "should 
 <span class='centered'>be" (keeping inverse images of open sets open) arises naturally from this 
 <span class='centered'>computational viewpoint, and many proofs of topology amount to finding 
 <span class='centered'>functional programs that fit a certain type. It's a great read, and I feel gave 
 <span class='centered'>me a much better sense of what topology is trying to do.  
 <ul><li><span class='centered'><span class='centered'>  <a href=http://bollu.github.io/#topology-is-really-about-computation--part-1><span class='centered'>I've written some exposition on this topic </a>, <span class='centered'>which maybe a more accessible read than the links above, since it tries to <span class='centered'>distill the fundamental idea into a blog post. </li></ul> 
 <h4><a id=articles/philosophy href='#articles/philosophy'> ยง </a><span class='centered'> Philosophy </h4> 
 <span class='centered'>I've wanted to understand philosophy as a whole for a while now, at least 
 <span class='centered'>enough to get a general sense of what happened in each century. The last year, 
 <span class='centered'>I meandered through some philosophy of science, which led me to some really 
 <span class='centered'>wild ideas (such as that of 
 <span class='centered'><a href=https://en.wikipedia.org/wiki/Epistemological_anarchism><span class='centered'>Paul Feyerabend's 'science as an anarchic enterprise' </a>
 <span class='centered'>which I really enjoyed).  
 <span class='centered'>I also seem to get a lot more out of audio and video than text in general, so 
 <span class='centered'>I've been looking for podcasts and video lectures. I've been following:  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://historyofphilosophy.net/><span class='centered'>The history of philosophy without any gaps </a><span class='centered'>for a detailed exposition on, say, the greeks, or the arabic philosophers. <span class='centered'>Unfortunately, this podcast focuses on far too much detail for me to have been <span class='centered'>able to use it as a way to get a broad idea about  <i><span class='centered'>philosophy </i> in itself. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://philosophizethis.org/><span class='centered'>Philosophize This! by Stephen West </a><span class='centered'>Is a good philosophy podcast for a  <i><span class='centered'>broad </i> overview of different areas <span class='centered'>of Philosophy. I got a lot out of this, since I was able to get a sense <span class='centered'>of the progression of ideas in (Western) Philosophy. So I now know what <span class='centered'><a href=https://plato.stanford.edu/entries/phenomenology/><span class='centered'>Phenomenology </a> is, <span class='centered'>or what Foucault was reacting against. </li></ul> 
 <span class='centered'>I also attempted to read a bunch of philosophers, but the only ones I could 
 <span class='centered'>make a good dent on were the ones listed below. I struggled in writing this 
 <span class='centered'>section, since it's much harder to sanity check my understanding of philosophy, 
 <span class='centered'>versus mathematics, since there seems to be a range of interpretations of the 
 <span class='centered'>same philosophical work, and the general imprecise nature of language doesn't 
 <span class='centered'>help here at all. So please take all the descriptions below with some salt 
 <span class='centered'>to taste.  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/Discipline_and_Punish><span class='centered'>Discipline and Punish by Michel Foucault </a><span class='centered'>Here, Foucault traces the history of the criminal justice system of society, <span class='centered'>and how it began as something performed 'on the body' (punishment), <span class='centered'>which was then expanded to a control 'of the mind' (reform). As usual, <span class='centered'>the perspective is fun, and I'm still going through the book. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/Madness_and_Civilization><span class='centered'>Madness and Civilization by Michel Foucault </a><span class='centered'>which attempts to chronicle how our view of madness evolved as society did. <span class='centered'>It describes how madmen, who were on the edges of society, but still <span class='centered'>"respected" (for exmaple, considered as 'being touched by the gods') were <span class='centered'>pathologized by the Renaissance, and were seen as requiring treatment. I'm <span class='centered'>still reading it, but it's enjoyable so far, as a new perspective for me. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/The_Value_of_Science><span class='centered'>The value of science by Henri Poincare </a>. <span class='centered'>Here, he defends the importance of experimentation, as well as the value of <span class='centered'>intuition to mathematics, along with the importance of what we consider <span class='centered'>formal logic. It's a tough read sometimes, but I think I got something out of <span class='centered'>it, at least in terms of perspective about science and mathematics. </li></ul> 
 <h4><a id=articles/information-theory href='#articles/information-theory'> ยง </a><span class='centered'> Information theory </h4> 
 <span class='centered'>I've been on a quest to understand information theory far better than I 
 <span class='centered'>currently do. In general, I feel like this might be a much better way to 
 <span class='centered'>internalize probability theory, since it feels like it states probabilistic 
 <span class='centered'>objects in terms of "couting" / "optimisation of encodings", which is a 
 <span class='centered'>perspective I find far more natural.  
 <span class='centered'>Towards this aim, I wound up reading:  
 <ul><li><span class='centered'><span class='centered'>  <a href=http://www.inference.org.uk/mackay/itila/book.html><span class='centered'>Information theory, Learning, and inference algorithms </a><span class='centered'>This book attempts to provide the holistic view I was hoping for. It has <span class='centered'>great illustrations of the basic objects of information theory. However, <span class='centered'>I was hoping that the three topics would be more "unified" in the book, <span class='centered'>rather than being presented as three separate sections with some amount <span class='centered'>of back-and-forth-referencing among them. Even so, it was a really fun read. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf><span class='centered'>Elements of information theory </a></li></ul> 
 <h4><a id=articles/building-intuition-for-sheaves-topoi-logic href='#articles/building-intuition-for-sheaves-topoi-logic'> ยง </a><span class='centered'> Building intuition for Sheaves, Topoi, Logic </h4> 
 <span class='centered'>I don't understand the trifecta of sheaves, topoi, geometry, and logic, and 
 <span class='centered'>I'm trying to attack this knot from multiple sides at once.  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1308.4621><span class='centered'>Understanding networks and their behaviours using sheaf theory </a></li><li><span class='centered'><span class='centered'>  <a href=https://www.sciencedirect.com/science/article/pii/S1571066108005264><span class='centered'>Sheaves, objects, and distributed systems </a></li><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1603.01446.pdf><span class='centered'>Sheaves are the canonical data strucure for sensor integration </a></li><li><span class='centered'><span class='centered'>  <a href=https://www.math.upenn.edu/~ghrist/EAT/EATchapter9.pdf><span class='centered'>Elementary applied topology, Chapter 9: Sheaves </a></li><li><span class='centered'><span class='centered'>  <a href=https://news.ycombinator.com/item?id=13677308><span class='centered'>Sheaf theory: The mathematics of data fusion (video) (link to HackerNews) </a></li></ul> 
 <span class='centered'>All of these provide geometric viewpoints of what sheaves are, in low-dimensional 
 <span class='centered'>examples of graphs which are easy to visualize. I'm also trudging through the 
 <span class='centered'>tome:  
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www.springer.com/gp/book/9780387977102><span class='centered'>Sheaves in geometry and logic: A first introduction to Topos theory </a></li></ul> 
 <span class='centered'>which appears to follow the "correct path" of the algebraic geometers, but this 
 <span class='centered'>requires a lot of bandwidth.  
 <ul><li><span class='centered'><span class='centered'>  <a href=http://math.stanford.edu/~vakil/216blog/FOAGnov1817public.pdf><span class='centered'>The Rising Sea: foundations of algebraic geometry by Ravi Vakil </a></li></ul> 
 <span class='centered'>This is a hardcore algebraic geometry textbook, and is arguably 
 <span class='centered'><i><span class='centered'>great for studying sheaves </i> because of it. Sheaves are Chapter 2, and allows 
 <span class='centered'>one to see them be developed in their "true setting" as it were. In that 
 <span class='centered'>Grothendeick first invented sheaves for algebraic geometry, so it's good to 
 <span class='centered'>see them in the home they were born in. Once again, this is a book I lack 
 <span class='centered'>bandwidth for except to breezily read it as I go to bed. I did get something 
 <span class='centered'>out from doing this. I'm considering taking this book up as an independent 
 <span class='centered'>study, say the first four chapters. I'll need someone who knows algebraic 
 <span class='centered'>geometry to supervise me, though, which is hard to find in an institute geared 
 <span class='centered'>purely for computer science. (If anyone on the internet is kind enough to 
 <span class='centered'>volunteer some of their time to answer questions, I'll be very glad! Please 
 <span class='centered'>email me at  <code class='inline'>rot13(fvqqh.qehvq@tznvy.pbz)</code>)  
 <h4><a id=articles/the-attic href='#articles/the-attic'> ยง </a><span class='centered'> The attic </h4> 
 <span class='centered'>This section contains random assortments that I don't recall how I stumbled 
 <span class='centered'>across, but too cool to not include on the list. I usually read these in bits 
 <span class='centered'>and pieces, or as bedtime reading right before I go to bed to skim.  I find 
 <span class='centered'>that skimming such things gives me access to knowing about tools I would not 
 <span class='centered'>have known otherwise. I like knowing the existence of things, even if I don't 
 <span class='centered'>recall the exact thing, since knowing that something like  <code class='inline'>X</code> exists has saved me 
 <span class='centered'>from having to reinvent  <code class='inline'>X</code> from scratch.  
 <ul><li><span class='centered'><span class='centered'>  <a href=http://www.cns.gatech.edu/GroupTheory/version9.0/GroupTheory.pdf><span class='centered'>Group Theory: Birdtracks, Lie's and Exceptional Groups by Predrag Cvitanovic </a><span class='centered'>is an exposition of Lie theory using some notation called as "Birdtrack notation", <span class='centered'>which is supposedly a very clean way of computing invariants, inspired by <span class='centered'>Feynmann notation. The writing style is informal and pleasant, and I decided <span class='centered'>to save the book purely because the first chapter begins with <span class='centered'>"Basic Concepts: A typical quantum theory is constructed from a few building blocks...". <span class='centered'>If a book considers building quantum theories as its starting point, I really <span class='centered'>want to see where it goes. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www.math.upenn.edu/~ghrist/notes.html><span class='centered'>Elementary Applied topology by Robert M Ghirst </a><span class='centered'>I wouldn't classify the book as elementary because it skims over too much to be <span class='centered'>useful as a reference, but it's great to gain an intuition for what, say, <span class='centered'>homology or cohomology is. I am currently reading the section on Sheaf theory, <span class='centered'>and I'm getting a lot out of it, since it describes how to write down, say, <span class='centered'>min-cut-max-flow or niquist-shannon in terms of sheaves. I don't grok it yet, <span class='centered'>but even knowing this can be done is very nice. The book is a wonderful <span class='centered'>walkthrough in general. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://icerm.brown.edu/video_archive/?play=2034><span class='centered'>On polysemous mathematical illustration by Robert M Ghirst </a><span class='centered'>This is a talk on the wonderful illustrations by the above author, about <span class='centered'>the different types of mathematical illustrations one can have, and different <span class='centered'>"levels of abstraction". </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://chronologia.org/en/math_impressions/poster016.html><span class='centered'>Mathematical Impressions: The illustrations of AT Femenko </a><span class='centered'>These are  <i><span class='centered'>beautiful </i> illustrated pictures of various concepts in math, which <span class='centered'>tend to  <i><span class='centered'>evoke </i> the feeling of the object, without being too direct about it. <span class='centered'>For example, consider "gradient descent" below. I highly recommend going <span class='centered'>through the full gallery. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://chronologia.org/art/math/123a176.jpg><span class='centered'>Gradient Descent </a><span class='centered'><img width=200 height=200 src="http://chronologia.org/art/math/123a176.jpg"> </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://chronologia.org/art/math/077a011.jpg><span class='centered'>Topological Zoo </a><span class='centered'><img width=200 height=200 src="http://chronologia.org/art/math/077a011.jpg"> </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://bastian.rieck.me/research/Dissertation_Rieck_2017.pdf><span class='centered'>Persistent Homology in Multivariate Data Visualization </a><span class='centered'>This is the PhD dissertation of  <a href=https://bastian.rieck.me/><span class='centered'>Bastian Rieck </a>, <span class='centered'>who's now a postdoc at ETH. I deeply enjoyed reading it, since it pays <span class='centered'>a lot of attention to the  <i><span class='centered'>design </i> of analyses, and how to interpret <span class='centered'>topological data. I really enjoyed getting a good sense of how one can <span class='centered'>use persistent homology to understand data, and the trade-offs between <span class='centered'><a href=https://en.wikipedia.org/wiki/Vietoris%E2%80%93Rips_complex><span class='centered'>Vietoris-Rips complex </a><span class='centered'>and the  <a href=https://en.wikipedia.org/wiki/%C4%8Cech_complex><span class='centered'>Cech complex </a>. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1205.5935.pdf><span class='centered'>An introduction to Geometric algebra </a><span class='centered'>I fell in love with geometric algebra, since it provides a really clean way <span class='centered'>to talk about  <i><span class='centered'>all possible subspaces </i> of a given vector space. This provides <span class='centered'>super slick solutions to many geometry and linear algebra problems. The <span class='centered'>way I tend to look at it is that when one does linear algebra, there's a strict <span class='centered'>separation between "vectors" (which are elements of the vector space), and, <span class='centered'>say, "hyperplanes" (which are  <i><span class='centered'>subspaces </i> of the vector space), as well as <span class='centered'>objects such as "rotations" (which are  <i><span class='centered'>operators </i> on the vector space). <span class='centered'>Geometric algebra provides a rich enough  <i><span class='centered'>instruction set </i> to throw all <span class='centered'>these three distinct things into a blender. This gives a really concise <span class='centered'>language to describe all phenomena that occurs in the vector space world --- <span class='centered'>which, let's be honest, is  <i><span class='centered'>most </i> tractable phenomena! I had a blast <span class='centered'>reading about GA and the kinds of operators it provides. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://arxiv.org/pdf/1807.07159><span class='centered'>Circuits via Topoi </a>. This paper attempts <span class='centered'>to provide an introduction to topos theory by providing a semantics for <span class='centered'>both combinational and sequential circuits under a unifying framework. I keep <span class='centered'>coming back to this article as I read more topos theory. Unfortunately, I'm <span class='centered'>not "there yet" in my understanding of topoi. I hope to be next year! </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://press.princeton.edu/books/paperback/9780691138718/fearless-symmetry><span class='centered'>Fearless Symmetry </a><span class='centered'>This is definitely my favourite non-fiction book that I've read in 2019, hands <span class='centered'>down. The book gives a great account of the mathematical objects that went <span class='centered'>into Wiles' book of Fermat's last theorem. It starts with things like <span class='centered'>"what is a permutation" and ends at questions like "what's a reciprocity law" <span class='centered'>or "what's the absolute galois group". While at points, I do believe the book <span class='centered'>goes far too rapidly, all in all, it's a solid account of number theory <span class='centered'>that's distilled, but not in any way diluted. I really recommend reading this <span class='centered'>book if you have any interest in number theory (or, like me, a passing <span class='centered'>distaste due to a course on elementary number theory I took, with proofs that <span class='centered'>looked very unmotivated). This book made me decide that I should, indeed, <span class='centered'>definitely learn algebraic number theory, upto at least <span class='centered'><a href=https://en.wikipedia.org/wiki/Artin_reciprocity_law><span class='centered'>Artin Reciprocity </a>. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/Remembrance_of_Earth%27s_Past><span class='centered'>Rememberance of Earth's past trilogy by Liu Cixin </a><span class='centered'>While I would not classify this as "mind-blowing" (which I do classify Greg <span class='centered'>Egan books as), they were still a solidly fun read into how humanity would <span class='centered'>evolve and interact with alien races. It also poses some standard solutions <span class='centered'>to the Fermi Paradox, but it's done well. I felt that the fact that it was <span class='centered'>translated was painfully obvious in certain parts of the translation, which <span class='centered'>I found quite unfortunate. However, I think book 3 makes up in grandeur for <span class='centered'>whatever was lost in translation. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://en.wikipedia.org/wiki/Walkaway_(Doctorow_novel><span class='centered'>Walkaway by Cory Doctorow </a>) <span class='centered'>The book is set in a dystopian nightmare, where people are attempting to <span class='centered'>"walk away" from society and set up communes, where they espouse having <span class='centered'>a post-scarcity style economy based on gifting. It was a really great <span class='centered'>description of what such a society could look like. I took issue with some <span class='centered'>weird love-triangle-like-shenanigans in the second half of the book, but <span class='centered'>the story arc more than makes up for it. Plus, the people throw a party <span class='centered'>called as a "communist party" in the first page of the book, which grabbed <span class='centered'>my attention immediately! </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://www.cs.unipr.it/purrs/><span class='centered'>PURRS: Parma University Recurrence Relation Solver </a><span class='centered'>I wanted better tactics for solving recurrences in Coq, which led me into <span class='centered'>a rabbit hole of the technology of recurrence relation solving. This was the <span class='centered'>newest  <i><span class='centered'>stable </i> reference to a complete tool that I was able to find. Their <span class='centered'>references section is invaluable, since it's been vetted by them <span class='centered'>actually implementing this tool! </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://www21.in.tum.de/~nipkow/TRaAT/><span class='centered'>Term rewriting and all that </a>. <span class='centered'>I read this book purely for its description of Groebner bases and the Bucchberger <span class='centered'>algorithm in a way that  <i><span class='centered'>made sense </i> for the first time. <span class='centered'><a href=http://bollu.github.io/#what-the-hell-is-a-grobner-basis-ideals-as-rewrite-systems><span class='centered'>I've written about this more extensively before </a><span class='centered'>so I'm not going to repeat myself here. In general, I think it's a great book <span class='centered'>that's worth reading, if nothing else, for at least the chapter on Groebner <span class='centered'>bases. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=http://worrydream.com/refs/Wadge%20-%20Lucid,%20the%20Dataflow%20Programming%20Language.pdf><span class='centered'>Lucid: The dataflow programming language </a><span class='centered'>This document is the user manual of Lucid. I didn't fully understand the <span class='centered'>book, but what I understood as their main argument is that full access too <span class='centered'>looping is un-necessary to perform most of the tasks that we do. Rather, <span class='centered'>one can provide a "rich enough" set of combinators to manipulate streams <span class='centered'>that allows one to write all programs worthy of our interest. </li></ul> 
 <ul><li><span class='centered'><span class='centered'>  <a href=https://hal.inria.fr/inria-00548290/document><span class='centered'>Bundle Adjustment โ A Modern Synthesis </a><span class='centered'>I learnt about Bundle Adjustment from a friend taking a course on robotics. <span class='centered'>The general problem is to reconstruct the 3D coordinates of a point cloud <span class='centered'>given 2D projections of the points and the camera parameters, as the camera <span class='centered'>moves in time. I found the paper interesting since it winds up invoking a <span class='centered'>decent amount of differential geometric and gauge theoretic language to <span class='centered'>describe the problem at hand. I was unable to see why this vocabulary helped <span class='centered'>in this use-case, but perhaps I missed the point of the paper. It was hard to <span class='centered'>tell. </li></ul> 
 <h4><a id=articles/conclusions href='#articles/conclusions'> ยง </a><span class='centered'> Conclusions </h4> 
 <span class='centered'>I always feel a little wrong posting this at the end of every year, since I 
 <span class='centered'>feel that among the things I cover under "read", I've internalized some things 
 <span class='centered'>far better than others: For example, I feel I understannd Riemannian geometry 
 <span class='centered'>far better than I do General Relativity. I try to put up the caveats at the 
 <span class='centered'>beginning of each section, but I'd really like a way to communicate my 
 <span class='centered'>confidence without reducing readability.  
 <span class='centered'>The final thing that I wish for is some kind of reading group? It's hard 
 <span class='centered'>to maintain a group when my interests shift as rapidly as they do, which 
 <span class='centered'>was one of the reason I really loved the AIRCS workshop: They were people 
 <span class='centered'>who were working on formal methods, compilers, type theory, number theory, 
 <span class='centered'>embedded systems, temporal logic... It was very cool to be in a group of 
 <span class='centered'>people who had answers and intuitions to questions that had bugged me for 
 <span class='centered'>some time now. I wonder if attending courses at a larger research university 
 <span class='centered'>feels the same way. My uni is good, but we have quite small population, which 
 <span class='centered'>almost by construction means reduced diversity.  
 <span class='centered'>I also wish that I could openly add more references to repos I've been working 
 <span class='centered'>on for a while now, but I can't due to the nature of academia and publishing. 
 <span class='centered'>This one bums me out, since there's a long story of a huge number of commits 
 <span class='centered'>and trial-by-fire that I think I'll be too exhausted to write about once the 
 <span class='centered'>thing is done.  
 <span class='centered'>Sometimes, I also wish that I could spend the time I spend reading  <i><span class='centered'>disparate </i>
 <span class='centered'>topics on  <i><span class='centered'>focused reading on one topic </i>. Unfortunately, I feel like I'm not 
 <span class='centered'>wired this way, and the joy I get from sampling many things at the same time 
 <span class='centered'>and making connections is somehow much deeper than the joy I get by deeply 
 <span class='centered'>reading one topic (in exclusion of all else). I don't know what this says 
 <span class='centered'>about my chances as a grad student in the future  <code class='inline'>:)</code>.  
 <script src="https://utteranc.es/client.js"        repo="bollu/bollu.github.io"        issue-term="pathname"        label="question"        theme="github-light"        crossorigin="anonymous"        async></script></container></body></html>