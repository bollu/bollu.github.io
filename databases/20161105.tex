\documentclass{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\begin{document}
\textbf{Siddharth Bhat (20161105)}

\section{Q1}
There is a component, call it A at site that initiated the transaction, and
components at each of the banks --- components B and C. Component B receives
the instruction from A. B checks that there is adequate money in the account, and
aborts if not.  Otherwise, B subtracts 10,000 from the account at B and
signals C to deposit 10,000 into the designated account at C. Component C
checks that the account exists, and aborts if not. Otherwise, C adds 10,000 to
the account and informs A of a successful conclusion. Once A has been
informed of the successful conclusion, it reports back to the user of
the success.

Due to the presence of time-outs, and since both $B$ and $C$ can send \texttt{Abort}
back to $A$, both crashes and failures such as insufficient funds are handled.
If $B$ or $C$ crash, the request times out; If there are insufficient funds,
then a \texttt{Abort} message is sent.
The coordinator sends a rollback message to all the participants.  Each
participant undoes their transactions using the undo log. Each participant sends an
acknowledgement to the coordinator.  The coordinator undoes the transaction
when all acknowledgements have been received.




\section{Q2}
\textbf{Consistency:} all nodes see the same data at the same time.
\textbf{Availability:} a guarantee that every request receives a response about whether it was successful or failed.
\textbf{Partition tolerance:} the system continues to operate despite arbitrary message loss.

The CAP theorem states that you cannot simultaneously have all three; you must make
tradeoffs among them. The CAP theorem is sometimes incorrectly described  as
the choice of picking consistency, availability, and partitioning during
\textbf{design time}. Really, the theorem allows for databases to choose
between these dynamically at \textbf{run time}.

When using NoSQL databases, we lose trade-off consistency for
a new property called \textbf{eventual consistency}.
This allows each system to make updates to data and learn of other updates
made by other systems within a short period of time, without being totally
consistent at all times. For most systems, knowing that it will reach
consistency in a short period of time is "good enough", when it allows
us to have consistency and availability as well!

\section{Q3}
\section{Q4}
\section{Q5}
\section{Q6}

When we insert $0011$, there are four records for bucket 0, which overflows.
Adding a second bit to the bucket addresses doesn't help, because the first
four records all begin with $00$. Thus, we go to $i = 3$ and use the first three
bits for each bucket address. Now we have space for $4$ records in each bucket;
however, we can get at maximum $2$ records per bucket --- we have 4 bit hashes,
our buckets are keyed by 3 bits, leaving 1 bit of ambiguity. 1 bit 
can be 2 records. When $1111$ is inserted, we have two records in each bucket.

\includegraphics[width=\textwidth, angle=270]{db-q6.pdf}

\section{Q7}
\subsubsection{Q7.1}
$SL_x(E)$ for a shared lock by transaction $x$ on elem $E$. $U_x(E)$ for
unlock by transaction $x$ on elem $E$.

\includegraphics[width=0.8\textwidth]{db-q71.pdf}

All locks are accepted, since there are no conflicting locks.

\subsubsection{Q7.2}

Table will be the same as for the above, since we did not have any read action
which was followed by a write action of the same element by the same
transaction.


\subsubsection{Q7.3}


\section{Q8}
\begin{align*}
    &r_1(O_1) \mapsto \text{$T_1$ puts }IS(B_1); S(O_1); \texttt{release} \\
    &r_2(O_2) \mapsto \text{$T_2$ puts }IS(B_1); S(O_2); \texttt{release}\\
    &r_3(O_1) \mapsto \text{$T_3$ puts }IS(B_2); S(O_3); \texttt{release}\\
    &w_1(O_3) \mapsto \text{$T_1$ puts }IX(B_2); X(O_3); \texttt{release}\\
    &w_2(O_4) \mapsto \text{$T_2$ puts }IX(B_2); X(O_4); \texttt{release}\\
    &w_3(O_5) \mapsto \text{$T_3$ puts }IX(B_2); X(O_5); \texttt{release}\\
    &w_1(O_2) \mapsto \text{$T_1$ puts }IX(B_2); X(O_3); \texttt{release}\\
\end{align*}
\section{Q9}
\subsubsection{9.1}
In Undo Logging Logging, we need to write all we need to write all modified
data to disk before committing a transaction.  This may need a large number of disk
I/Oâ€™s. This is unlike the case of Redo logging, which allows changes to be
present in-memory; only need to flush changes before committing.

\subsubsection{9.2}
Selinger optimization improves upon DP approach by keeping for
each  not only the plan of least cost, but also plans that have higher
cost but produce a result that is sorted in an order that may
be useful for parent queries.
\subsubsection{9.3}

View serializable: If a given schedule is found to be view equivalent to some serial schedule. Alternatively,
  there are no cycles in the dependency graph.
Conflict serializable: If there are no cycles in the conflict graph.


\subsubsection{9.4}

We can use strict 2-phase locking for recoverability. This requires that
in addition to the lock being 2-Phase, all Exclusive(X) Locks held by the
transaction be released until after the Transaction Commits.

\subsubsection{9.5}
Database operations are in fact relational algebra operations. These
operations are pure mathematical expressions, and are generally reads or
writes into disjoint pieces of data. This makes them naturally parallelizable.


\subsubsection{9.6}
File system does not generally have multiple readers and writers to a single
file. It also does not need to manage structured data. Hence, many of the ACID
like concerns simply do not occur in the case of a file system.

\subsubsection{9.7}
the commit bit for X is true if and only if the most recent
transaction to write X has already committed. The purpose of this bit
is to avoid a situation where one transaction T reads data written by
another transaction U, and U then aborts. This problem, where T makes
a "dirty read" of uncommitted data, certainly can cause the database
state to become inconsistent, and any scheduler needs a mechanism to
prevent dirty reads.

\subsubsection{9.8}
\begin{itemize}
        \item  Two-phase locking - 2PL.
        \item General lock based solutions.
        \item Timestamp ordering.
        \item Validation based concurrency control.
\end{itemize}

Increment based locking is good in this case because it allows to add or subtract a constant
from an element, which is what most kinds of bank transaactions are. Increment locks on the same
element do not conflict with each other.
\subsubsection{9.9}
recovery manager will have to DODO
\subsubsection{9.10}
all trees of n vertices is $n^{n-2}$. Number of left-deep trees is $n!$.
$n^{n-2}$ is much larger than $n!$.


\end{document}
