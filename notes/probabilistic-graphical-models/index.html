<h1 id="background-and-aims">Background and aims</h1>
<p>Consider a distribution of binary random variables <span class="math inline">\(x_1, x_2, \dots x_n, y\)</span>. Note that to define the value of <span class="math inline">\(P(x_1)\)</span>, we need just one value: <span class="math inline">\(P(x_1 = 0)\)</span>. We can derive <span class="math inline">\(P(x_1 = 1) \equiv 1 - P(x_1 = 0)\)</span>.</p>
<p>However, the full joint distribution <span class="math inline">\(P(x_1, x_2, \dots, x_n, y)\)</span> needs <span class="math inline">\(2^{n+1} - 1\)</span> values to fully define.</p>
<p>However, let us assume that <span class="math inline">\(P(x_i|y)\)</span> are all independent. Hence, we can rewrite the above distribution as <span class="math inline">\(P(y) \prod_{i=1}^n P(x_i|y)\)</span>. Now, we need to know <span class="math inline">\(P(x_i|y=0), P(x_i|y=1)\)</span>. Both of these are binary random variables which need one value to define. So in toto, we need <span class="math inline">\(2n + 1\)</span> values for the above (factored) joint distribution.</p>
<p>So, we will study how to represent, perform inference, and perfom bayesian updates (learning). Also, connections to boltzmann distributions and whatnot will be explored. Connections to graph theory as well. We are also going to study MCMC (Markov chain monte carlo) methods. I hope we study more than just metropolis hastings: I want to understand Hamiltonian and Lavengin Monte Carlo more deeply (NUTS sampling, slice sampling, their interactions with HMC, etc). Later, we will see some connections to Learning theory (PAC learning - defined by Valiant).</p>
<p>The textbook is “Kohler and Friedman”.</p>
<h3 id="a-teaser-problem">A teaser problem</h3>
<p>We start with an ordered deck. We propose a shuffling mechanism: take the top card and move it to somewhere in the deck. Eg. If we start form <span class="math inline">\((1, 2, 3)\)</span>, we can move this to <span class="math inline">\((2, 1, 3)\)</span>, or <span class="math inline">\((2, 3, 1)\)</span>. Now, when the card <span class="math inline">\(3\)</span> comes to the top, note that we had placed all other numbers in the deck with uniform probability. So, when the card <span class="math inline">\(3\)</span> comes to the top, all the other cards are uniformly distributed. We now need to place <span class="math inline">\(3\)</span> uniformly in the deck.</p>
<p>Let <span class="math inline">\(T_1\)</span> be the random variable of the first round at which a single card is placed <em>underneath</em> <span class="math inline">\(n\)</span>.</p>
<p>There are <span class="math inline">\(n-1\)</span> slots where can place any top card, so the likelihood of hitting the bottom slot is <span class="math inline">\(1/(n-1)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
&amp;P(T_1=1) \equiv \frac{1}{n - 1} \\
&amp;P(T_1=2) \equiv \left(1 - \frac{1}{n-1}\right) \frac{1}{n-1} = \frac{n-2}{n-1} \\
&amp;P(T_1=i) \equiv \left(1 - P(T_1=i-1) \right) \frac{1}{n-1} = 
    \left( 1 - \frac{1}{n-1} \right)^{i-1} \frac{1}{n-1} = \frac{(n-2)^{i-1}}{(n-1)^i}\end{aligned}\]</span></p>
<p>This is a geometric distribution with parameter <span class="math inline">\(\frac{1}{n-1}\)</span>. The expectation is going to be <span class="math inline">\({\ensuremath{\mathbb{E} \left[ T_1 \right]}} \equiv n-1\)</span>.</p>
<p>We now define <span class="math inline">\(T_2\)</span> to be the random variable which is the time from when the first card went underneath the <span class="math inline">\(n\)</span>th card, to when the second card went underneath the <span class="math inline">\(n\)</span>th card. We have two locations at the bottom. Eg. if we had <span class="math inline">\((1, 2, 3, 4)\)</span> to start with, and after <span class="math inline">\(T_1\)</span>, we are now at <span class="math inline">\((2, 3, 4, 1)\)</span>. We now have two positions <span class="math inline">\((2, 3, 4, \circ, 1, \circ)\)</span> to be underneath the card <span class="math inline">\(4\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
&amp;P(T_2=1) \equiv \frac{2}{n - 1} \\
&amp;P(T_2=i) \equiv \left( 1 - \frac{2}{n-1} \right)^{i-1} \frac{2}{n-1}\end{aligned}\]</span></p>
<p>This is a geometric distribution with parameter <span class="math inline">\(\frac{2}{n-1}\)</span>. The expectation is going to be <span class="math inline">\({\ensuremath{\mathbb{E} \left[ T_1 \right]}} \equiv n-2\)</span>.</p>
<p>The total time for the <span class="math inline">\(n\)</span>th card to reach the top is going to be <span class="math inline">\(T \equiv T_1 + T_2 + \dots + T_n\)</span>. So the expectation is going to be <span class="math inline">\({\ensuremath{\mathbb{E} \left[ T \right]}} = \sum_i {\ensuremath{\mathbb{E} \left[ T_i \right]}} = \sum_i \frac{1}{n-i}\)</span></p>
<h3 id="another-problem">Another problem</h3>
<p>There are three balls, numbered <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span>, and there are three numbered bins. We throw the first ball into each of the three bins with equal probability. Independently, throw the second ball and the third ball to each of the three bins Independently.</p>
<p>Let <span class="math inline">\(X\)</span> be the number of balls in the first balls in the first bin. Let <span class="math inline">\(N\)</span> be the number of non-empty bins.</p>
<p>Write down <span class="math inline">\(P(X), P(N), P(X, N)\)</span> where <span class="math inline">\(P(X, N)\)</span> is the joint distribution. Also find <span class="math inline">\(P(X|N)\)</span>, <span class="math inline">\(P(N|X)\)</span>.</p>
<h1 id="review-of-probability">Review of Probability</h1>
<p>The first thing we should know is the sample space <span class="math inline">\(\Omega\)</span>. Next, we care about events <span class="math inline">\(E \subseteq {\ensuremath{\mathcal{P}}}(\Omega)\)</span>. The probability is a function <span class="math inline">\(P: \Omega \rightarrow {\ensuremath{\mathbb R_{\geq 0}}}\)</span> such that:</p>
<ul>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
<li><p>if <span class="math inline">\(A \cap B = \emptyset\)</span>, then <span class="math inline">\(P(A \cap B) = P(A) + P(B)\)</span></p></li>
</ul>
<p>A random variable is a function <span class="math inline">\(X: \Omega \rightarrow {\ensuremath{\mathbb R}}\)</span>.</p>
<p>The expectation of a random variable <span class="math inline">\({\ensuremath{\mathbb{E} \left[ X \right]}} \equiv \sum_i i \cdot P(X=i)\)</span>. That is, it is the average.</p>
<p>Markov’s inequality says that <span class="math inline">\({\ensuremath{\mathbb{P} \left[ X &gt; a \right]}} \leq {\ensuremath{\mathbb{E} \left[ X \right]}}/a\)</span>.</p>
<p>Chebyshev’s inequality says that <span class="math inline">\({\ensuremath{\mathbb{P} \left[ |X - \mu| &gt; a \right]}} \leq Var(X)/a^2\)</span>.</p>
<h2 id="conditional-probability">Conditional Probability</h2>
<p>To control to sample space and smooth interpolate probabilities a we restrict to a smaller sample space, we use <em>conditioning</em>. Hence we define the conditional probability: <span class="math inline">\(P(B|A) \equiv P(B \cap A) / P(A)\)</span></p>
<p>We can have a chain of conditioning: <span class="math display">\[\begin{aligned}
    P(A \cap B \cap C) = P(A) P(B \cap C | A) = P(A) P(B|A) P(C|A \cap B)\end{aligned}\]</span></p>
<p>We have 3 bins and 4 balls. We throw the 4 balls into the 3 bins independently. Each ball will land into some bin.</p>
<p>Let <span class="math inline">\(X\)</span> be the number of balls in bin 1. <span class="math inline">\(N\)</span> be the number of non-empty bins.</p>
<p>The total number of possibilities is <span class="math inline">\(3^4\)</span>, since each ball has <span class="math inline">\(3\)</span> possible bins, and there are <span class="math inline">\(4\)</span> independent balls.</p>
<p><span class="math inline">\(P(X = 2 \land N = 1) = 0\)</span>. We have have only two balls in bin <span class="math inline">\(1\)</span> which occupies one bucket. Hence,the other two balls need another bin. <span class="math inline">\(N\)</span> can be at minimum <span class="math inline">\(2\)</span>.</p>
<p><span class="math display">\[P(X = 2 \land N = 2) = 
    \text{ways to send 2 balls to 1st bin}
    \cdot
    \text{leftover bin to send the leftover 2 balls} = 
    \frac{ {4 \choose 2} \cdot 2 }{3^4}\]</span></p>
<p>We have a drunken man who takes a step forward with <span class="math inline">\(p = 1/2\)</span>, and 2 steps backward with probability <span class="math inline">\(p = 1/2\)</span>. at time <span class="math inline">\(t = 0\)</span>, <span class="math inline">\(x = 0\)</span>. Let <span class="math inline">\(T\)</span> be the time of passing out. Let <span class="math inline">\(pos_T\)</span> be the set of positions man could have been in when he passes out at time <span class="math inline">\(T\)</span>.</p>
<p>For example, at <span class="math inline">\(T = 0, pos_0 = \{ 0 \}\)</span>. At <span class="math inline">\(T = 1, pos_1 = \{ -2, 1 \}\)</span>. At <span class="math inline">\(T = 2, pos_2 = \{ -4, -1, 2 \}\)</span>. <span class="math inline">\(pos_3 = \{ 0, \dots \}\)</span>.</p>
<p>Let <span class="math inline">\(Y\)</span> be a random variable such that <span class="math inline">\(Y\)</span> is <span class="math inline">\(1\)</span> with probability <span class="math inline">\(0.5\)</span>, and <span class="math inline">\(-2\)</span> with probability <span class="math inline">\(0.5\)</span>. Let <span class="math inline">\(X\)</span> be the random variable that is the position after <span class="math inline">\(T\)</span> steps. For example, <span class="math inline">\({\ensuremath{\mathbb{E} \left[ X | T = 1 \right]}} = {\ensuremath{\mathbb{E} \left[ Y \right]}}\)</span>, <span class="math inline">\({\ensuremath{\mathbb{E} \left[ X | T = 2 \right]}} = {\ensuremath{\mathbb{E} \left[ Y + Y \right]}} = {\ensuremath{\mathbb{E} \left[ Y \right]}} + {\ensuremath{\mathbb{E} \left[ Y \right]}}\)</span>. Linearity of distribution saves us here.</p>
<p>We have two distributions on the same sample sample, <span class="math inline">\(p, q: \Omega \rightarrow [0, 1]\)</span>. We need to distinguish between <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. We have an oracle <span class="math inline">\(O\)</span> that provides numbers distributed according to either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span>. That is, we are given access to <span class="math inline">\(O_r\)</span> which provides numbers distributed according to distribution <span class="math inline">\(r\)</span>. Return whether <span class="math inline">\(r = p\)</span> or <span class="math inline">\(r = q\)</span>.</p>
<p>We should probably look at the event <span class="math inline">\(a \equiv \max_{A \subseteq \Omega} p(A) - q(A)\)</span>, the element that exhibits maximum discrepancy. Then we should draw events from the set which maximised <span class="math inline">\(a\)</span> and see what happens.</p>
<p>Now, this number <span class="math inline">\(a\)</span> happens to be equal to <span class="math inline">\(\frac{1}{2} \sum_{w \in \Omega}  |p(w) - q(w)|\)</span></p>
<h1 id="belief-networks">Belief networks</h1>
<p>There are two people, Alice and Bob. They are neighbours, and there’s a wall between their houses. Alice’s house has a sprinkler. One day, she wakes up and notices that her lawn is wet. So now, we have the random variables:</p>
<ul>
<li><p><span class="math inline">\(A\)</span>: Alice’s lawn is wet/not wet.</p></li>
<li><p><span class="math inline">\(B\)</span>: Bob’s lawn is wet/not wet.</p></li>
<li><p><span class="math inline">\(S\)</span>: Alice’s sprinkler was switched on/not switched on.</p></li>
<li><p><span class="math inline">\(R\)</span>: Rained or not.</p></li>
</ul>
<p>We have a joint probability distribution <span class="math inline">\(P(A~ B~ S~ R)\)</span>. We’re now going to factor this using Bayes rule: <span class="math display">\[\begin{aligned}
    &amp;{\ensuremath{\mathbb{P} \left[ A|BSR \right]}} {\ensuremath{\mathbb{P} \left[ BSR \right]}} = {\ensuremath{\mathbb{P} \left[ A|BSR \right]}} {\ensuremath{\mathbb{P} \left[ B |SR \right]}} {\ensuremath{\mathbb{P} \left[ R|S \right]}} {\ensuremath{\mathbb{P} \left[ S \right]}}\end{aligned}\]</span></p>
<p>For <span class="math inline">\({\ensuremath{\mathbb{P} \left[ A|BSR \right]}}\)</span> we need to provide <span class="math inline">\({\ensuremath{\mathbb{P} \left[ A=0|B=b~ S=s~ R=r \right]}}\)</span>. We can calculate <span class="math display">\[{\ensuremath{\mathbb{P} \left[ A=1|B=b~S=s~R=r \right]}} = 1 - {\ensuremath{\mathbb{P} \left[ A=0|B=b~ S=s~ R=r \right]}}\]</span> So we need to provide <span class="math inline">\(8\)</span> values for all possible choices of <span class="math inline">\((b, s, r)\)</span>.</p>
<p>Similarly, for <span class="math inline">\({\ensuremath{\mathbb{P} \left[ B|SR \right]}}\)</span> we need 4 numbers, and <span class="math inline">\({\ensuremath{\mathbb{P} \left[ R|S \right]}}\)</span> we need 2 numbers, and <span class="math inline">\({\ensuremath{\mathbb{P} \left[ S \right]}}\)</span> we need 1 number. The total is <span class="math inline">\(8 + 4 + 2 + 1 = 15\)</span>, which is how many we need to describe the <em>full</em> joint distribution <span class="math inline">\(P(A, B, S, R)\)</span>.</p>
<p>Now, for example, if we know the status of rain, we don’t <em>need to know the status of Bob’s lawn to comment on Alice’s lawn</em>. So, we can rewrite <span class="math inline">\(P(A|BSR) = P(A|SR)\)</span>. This now needs only <span class="math inline">\(4\)</span> variables, from <span class="math inline">\(8\)</span>. Similarly, if we know the status of the rain, the sprinkler in Alice’s yard cannot affect Bob’s yard. So, we can rewrite <span class="math inline">\(P(B|SR) = P(B|R)\)</span>. This dropped 4 numbers to 2 numbers. Finally, the status of rain does not affect whether the sprinkler was on or not. Hence, <span class="math inline">\(P(S|R) = P(S)\)</span>. This lowers 2 numbers to 1 numbers. Hence, we now have a total of <span class="math inline">\(4 + 2 + 1 + 1 = 8\)</span>, which is <em>half</em> of what we started with.</p>
<p>We can draw our relationships as a graph:</p>
<p>Let’s assume we have an instantiation of this model on some concrete numbers:</p>
<table>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ R=1 \right]}} = 0.2\)</span></td>
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ S=1 \right]}} = 0.1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ B=1|R=1 \right]}} = 1\)</span></td>
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ B=1|R=0 \right]}} = 0.2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ A=1|R=0~S=0 \right]}} = 0\)</span></td>
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ A=1|R=1~S=1 \right]}} = 1\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ A=1|R=1~S=0 \right]}} = 1\)</span></td>
<td align="left"><span class="math inline">\({\ensuremath{\mathbb{P} \left[ A=1|R=0~S=1 \right]}} = 0.9\)</span></td>
</tr>
</tbody>
</table>
<p>We can now begin number-crunching and compute what <span class="math inline">\({\ensuremath{\mathbb{P} \left[ S=1|A=1 \right]}}\)</span> is going to be: <span class="math display">\[\begin{aligned}
{\ensuremath{\mathbb{P} \left[ S=1|A=1 \right]}} &amp;= \frac{{\ensuremath{\mathbb{P} \left[ S=1~A=1 \right]}}}{{\ensuremath{\mathbb{P} \left[ A=1 \right]}}} \\
&amp;= \frac{\sum_{R~ B} {\ensuremath{\mathbb{P} \left[ S=1~ A=1~ R~ B \right]}}}{\sum_{R~ B~ S} {\ensuremath{\mathbb{P} \left[ A=1~ R~ B~ S \right]}}} \\
&amp;= \frac{\sum_{R~ B} {\ensuremath{\mathbb{P} \left[ A=1 |R~ S=1 \right]}}{\ensuremath{\mathbb{P} \left[ B | R~ S=1 \right]}} {\ensuremath{\mathbb{P} \left[ R \right]}} {\ensuremath{\mathbb{P} \left[ S \right]}}}{\sum_{R~ B~ S} {\ensuremath{\mathbb{P} \left[ A=1|RSB \right]}} {\ensuremath{\mathbb{P} \left[ B|RS \right]}} {\ensuremath{\mathbb{P} \left[ R \right]}} {\ensuremath{\mathbb{P} \left[ S \right]}}}\end{aligned}\]</span></p>
<p>Suppose we find that <span class="math inline">\({\ensuremath{\mathbb{P} \left[ S=1|A=1 \right]}} = 0.3382\)</span>. Now, what is <span class="math inline">\({\ensuremath{\mathbb{P} \left[ S=1|A=1~B=1 \right]}}\)</span>?</p>
<p><span class="math display">\[\begin{aligned}
{\ensuremath{\mathbb{P} \left[ S=1|A=1~B=1 \right]}} &amp;= \frac{{\ensuremath{\mathbb{P} \left[ S=1~A=1~B=1 \right]}}}{{\ensuremath{\mathbb{P} \left[ A=1~B=1 \right]}}} \\
&amp;= \frac{\sum_{R~} \dots} {\sum_{S, R} {\ensuremath{\mathbb{P} \left[ A=1~B=1~S~R \right]}}}\end{aligned}\]</span></p>
<p>We find that <span class="math inline">\(P(S=1|A=1~B=1) = 0.1604\)</span>. That is, if both alice and bob had wet lawns, then it’s unlikely that the water was from the sprinkler.</p>
<p>Now, how do we design automated algorithms such that the above calculations we performed can be done <em>automatically</em> and <em>quickly</em>?</p>
<h2 id="computing-joint-distributions-from-the-graph-of-the-belief-net">Computing joint distributions from the graph of the belief net</h2>
<p>We first perform a topological sort of the DAG(directed acyclic graph), which is always guaranteed to exist.</p>
<p>We now visit the topo sort and multiply the values corresponding to each of the factors.</p>
<p>Notice that <span class="math inline">\(R\)</span> has no incoming edges, so we need a term of <span class="math inline">\({\ensuremath{\mathbb{P} \left[ R \right]}}\)</span>, Similarly, <span class="math inline">\(S\)</span> has no incoming edges, so we need a term of <span class="math inline">\({\ensuremath{\mathbb{P} \left[ S \right]}}\)</span>. <span class="math inline">\(B\)</span> has an incoming edges from <span class="math inline">\(R\)</span>, so we get a term <span class="math inline">\({\ensuremath{\mathbb{P} \left[ B|R \right]}}\)</span>. Similarly, <span class="math inline">\(A\)</span> has incoming edges from <span class="math inline">\(R, S\)</span> so we get a term <span class="math inline">\({\ensuremath{\mathbb{P} \left[ A|S~ R \right]}}\)</span>. This gives us the final solution:</p>
<p><span class="math display">\[{\ensuremath{\mathbb{P} \left[ A~B~S~R \right]}} = {\ensuremath{\mathbb{P} \left[ R \right]}}{\ensuremath{\mathbb{P} \left[ S \right]}}{\ensuremath{\mathbb{P} \left[ B|R \right]}}{\ensuremath{\mathbb{P} \left[ A|S~R \right]}}\]</span></p>
<h2 id="conditional-independence-of-random-variables">Conditional independence of random variables</h2>
<p>We wish to study situations of the form: “Are random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent conditioned on <span class="math inline">\(C\)</span>”? If they are, this is notated as: <span class="math display">\[\begin{aligned}
    X \perp Y | C \iff {\ensuremath{\mathbb{P} \left[ X~ Y|C \right]}} = {\ensuremath{\mathbb{P} \left[ X|C \right]}} {\ensuremath{\mathbb{P} \left[ Y|C \right]}}\end{aligned}\]</span></p>
<p>Let us assume we have <span class="math inline">\(x_1, x_2, x_3\)</span>, all of which are dependent on each other with a network of the form:</p>
<p>There are 6 graphs that can represent the network (<span class="math inline">\(3!=6\)</span>)</p>
<ul>
<li><p>1. <span class="math inline">\({(x_1 \rightarrow x_3 \leftarrow x_2)}\)</span>. We get <span class="math inline">\({\ensuremath{\mathbb{P} \left[ X_1~X_2~X_3 \right]}} = {\ensuremath{\mathbb{P} \left[ X_1 \right]}}~{\ensuremath{\mathbb{P} \left[ X_2 \right]}}~{\ensuremath{\mathbb{P} \left[ X_3|X_1~X_2 \right]}}\)</span></p></li>
<li><p>2. <span class="math inline">\({(x_1 \leftarrow x_3 \rightarrow x_2)}\)</span>. We get <span class="math inline">\({\ensuremath{\mathbb{P} \left[ X_1~X_2~X_3 \right]}} = {\ensuremath{\mathbb{P} \left[ X_3 \right]}}~{\ensuremath{\mathbb{P} \left[ X_1|X_3 \right]}}~{\ensuremath{\mathbb{P} \left[ X_2|X_3 \right]}}\)</span></p></li>
<li><p>3. <span class="math inline">\({x_1 \rightarrow x_3 \rightarrow x_2}\)</span>. We get <span class="math inline">\({\ensuremath{\mathbb{P} \left[ X_1~X_2~X_3 \right]}} = {\ensuremath{\mathbb{P} \left[ X_1 \right]}} {\ensuremath{\mathbb{P} \left[ X_3 | X_1 \right]}} {\ensuremath{\mathbb{P} \left[ X_2|X_3 \right]}}\)</span>.</p></li>
</ul>
<p>Note that the first graph is not the same as the second graph, since in the first graph, <span class="math inline">\((X_1, X_2)\)</span> are independent, but in the second one, they are not.</p>
<p>The second and third are equal, and we can prove it! <span class="math display">\[\begin{aligned}
    1 \leftarrow 3 \rightarrow 2 &amp;= ({\ensuremath{\mathbb{P} \left[ X_3 \right]}}~{\ensuremath{\mathbb{P} \left[ X_1|X_3 \right]}})~{\ensuremath{\mathbb{P} \left[ X_2|X_3 \right]}}  \\
       &amp;=   ({\ensuremath{\mathbb{P} \left[ X_1 \right]}}~{\ensuremath{\mathbb{P} \left[ X_3|X_1 \right]}})~{\ensuremath{\mathbb{P} \left[ X_2|X_3 \right]}} \text{(Bayes rule)} \\
       &amp;= 1 \rightarrow 3 \rightarrow 2\end{aligned}\]</span></p>
<p>Similarly, the third and fourth are equal: <span class="math display">\[\begin{aligned}
    2 \rightarrow 3 \rightarrow 1 &amp;= ({\ensuremath{\mathbb{P} \left[ X_2 \right]}}~{\ensuremath{\mathbb{P} \left[ X_3|X_2 \right]}})~{\ensuremath{\mathbb{P} \left[ X_1|X_3 \right]}}  \\
       &amp;=   ({\ensuremath{\mathbb{P} \left[ X_3 \right]}}~{\ensuremath{\mathbb{P} \left[ X_3|X_2 \right]}})~{\ensuremath{\mathbb{P} \left[ X_1|X_3 \right]}} \text{(Bayes rule)} \\
       &amp;= 2 \leftarrow 3 \rightarrow 1\end{aligned}\]</span></p>
<p>subgraphs of the form <span class="math inline">\({(x_1 \rightarrow x_3 \leftarrow x_2)}\)</span> are called as <strong>collisions</strong>. Here, <span class="math inline">\(x_1, x_2\)</span> are independent, but conditioned on <span class="math inline">\(x_3\)</span>, they may become dependent!</p>
<p><span class="math display">\[\begin{aligned}
 &amp;{\ensuremath{\mathbb{P} \left[ X_1~ X_2 | X_3 \right]}} =_? {\ensuremath{\mathbb{P} \left[ X_1 | X_3 \right]}} {\ensuremath{\mathbb{P} \left[ X_2 | X_3 \right]}} \\
 &amp;{\ensuremath{\mathbb{P} \left[ X_1|X_3 \right]}}{\ensuremath{\mathbb{P} \left[ X_2 | X_1 ~X_3 \right]}} =_? {\ensuremath{\mathbb{P} \left[ X_1 | X_3 \right]}} {\ensuremath{\mathbb{P} \left[ X_2 | X_3 \right]}} \\
 &amp;{\ensuremath{\mathbb{P} \left[ X_2 | X_1 ~X_3 \right]}} =_? {\ensuremath{\mathbb{P} \left[ X_2 | X_3 \right]}} \\\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(X_1, X_2\)</span> be binary random variables that take on values <span class="math inline">\({0, 1}\)</span> with probability half. Let <span class="math inline">\(X_3 = X_1 \oplus X_2\)</span> (<span class="math inline">\(\oplus\)</span> represents XOR). Now, <span class="math inline">\(X_3\)</span> is also a random variable. Now, notice that if we know only <em>one</em> random variable, the other two random variables are independent. However, as soon as we know <em>two random variables</em>, the third one is completely determined! So, <span class="math inline">\({\ensuremath{\mathbb{P} \left[ X_1 | X_1 ~ X_3 \right]}} \in \{0, 1\}\)</span>, while <span class="math inline">\({\ensuremath{\mathbb{P} \left[ X_2 | X_3 \right]}} = 0.5\)</span> for this example. Hence, the above probabilities are not equal, and therefore <span class="math inline">\(X_1, X_2\)</span> are not independent when conditioned on <span class="math inline">\(X_3\)</span>.</p>
<h2 id="d-connectivity">d connectivity</h2>
<p>We say that <span class="math inline">\(X, Y\)</span> are d-connected wrt <span class="math inline">\(Z\)</span>, where <span class="math inline">\(Z\)</span> is a set of random variables, if there exists an <em>undirected path</em> such that:</p>
<ul>
<li><p>For all colliders, either <span class="math inline">\(C\)</span> or a descendant of <span class="math inline">\(C \in Z\)</span>.</p></li>
<li><p>No non-colliders should be in <span class="math inline">\(Z\)</span>.</p></li>
</ul>
<p><span class="math inline">\(a \rightarrow b \rightarrow d \leftarrow c\)</span>. Now let’s consider whether <span class="math inline">\(a, c\)</span> are d-connected with respect to <span class="math inline">\(d\)</span>. There exists a path <span class="math inline">\(a \rightarrow b \rightarrow d \leftarrow c\)</span>. In this path, <span class="math inline">\(d\)</span> is a colliding vertex, which does belong to <span class="math inline">\(Z\)</span>. <span class="math inline">\(b\)</span> is a non-colliding vertex that does not belong to <span class="math inline">\(Z\)</span>. Hence, this satisfies the conditions for this to be a <span class="math inline">\(d\)</span>-connected path.</p>
<p><span class="math inline">\(\text{\)</span>X<span class="math inline">\( is not d-connected to \)</span>Y<span class="math inline">\( with respect to \)</span>Z<span class="math inline">\(} \implies (X \perp Y | C)\)</span> Note that this is not iff.</p>
<h1 id="belief-bayesian-nets">Belief / Bayesian nets</h1>
<p>Missed class</p>
<h1 id="markov-networks">Markov Networks</h1>
<p>Not all distributions which have conditional independences. Not all of them can be modeled by a Bayes net.</p>
<p>If we have an undirected graph <span class="math inline">\(G\)</span> where every person chooses the color of their hair based on their neighbour, and we have a 4-cycle <span class="math inline">\(A \rightarrow B \rightarrow C \rightarrow D \rightarrow A\)</span>, then we can build a bayes net:</p>
<p><span class="math display">\[\begin{aligned}
&amp;x_a \rightarrow x_c \\
&amp;x_a \rightarrow x_b \\
&amp;x_b \rightarrow x_d \\
&amp;x_c \rightarrow x_d \\\end{aligned}\]</span></p>
<p>However, note that our bayes net is directed, while the underlying process was undirected. I can build the same bayes net by starting with <span class="math inline">\(x_c\)</span>: <span class="math display">\[\begin{aligned}
&amp;x_c \rightarrow x_a \\
&amp;x_c \rightarrow x_d \\
&amp;x_a \rightarrow x_b \\
&amp;x_d \rightarrow x_b \\\end{aligned}\]</span></p>
<p>According to this, <span class="math inline">\(X_C \texttt{independent} X_B | X_A, X_D\)</span>, but this is not true in the first bayes net!</p>
<p>To fix this, we just draw the <em>undirected graph</em>. For this, we need a tool called as a <em>potential</em>: potentials are non-negative functions that are associated with cliques in the graph, denoted by <span class="math inline">\(\phi\)</span>.</p>
<p>A clique of a graph is a complete sub-graph of a graph.</p>
<p><span class="math display">\[\begin{aligned}
&amp;a, b \in \{ r, b, g \} \\
&amp;\phi_{A, B}(a, b) \equiv \begin{cases} 10 &amp; a \neq b \\ 1 &amp; a = b\end{cases}\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
    P_{X_A X_B X_C X_D} \equiv \prod_{c \in C} \phi_c (x_c) = 
    \frac{1}{Z} \phi_{AB}(a, b) \phi_{AC}(a, c) \phi_{CD}(c, d) \phi_{BD}(b, d)\end{aligned}\]</span></p>
<p>Here, <span class="math inline">\(Z\)</span> is called as the partition function. It is the normalizing constant: <span class="math inline">\(Z \equiv \sum_{a, b, c, d} \phi_{AB}(a, b) \phi_{AC}(a, b) \phi_{CD}(c, d) \phi_{BD}(b, d)\)</span>.</p>
<h1 id="preliminary-definitions-of-information">Preliminary definitions of Information</h1>
<p>Entropy(<span><span class="math inline">\(H\)</span></span>): The entropy of a random variable <span class="math inline">\(X\)</span> with probability distribution <span class="math inline">\(p: X \rightarrow {\ensuremath{\mathbb R}}\)</span> is defined as: <span class="math display">\[\begin{aligned}
        {\ensuremath{H}}(X) \equiv - \sum_{x \in X} p(x) \log p(x) = {\ensuremath{\mathbb{E} \left[ [ \right]}}- \log \circ p]
    \end{aligned}\]</span></p>
<p>Condtional entropy(<span class="math inline">\({\ensuremath{H}}(X|Y)\)</span>): The conditional entropy of a random variable <span class="math inline">\(X\)</span> with respect to another variable <span class="math inline">\(Y\)</span> is defined as: <span class="math display">\[\begin{aligned}
        {\ensuremath{H}}(X|Y) \equiv &amp;- \sum_{y \in Y} p(y) {\ensuremath{H}}(X|Y=y) \\
                       &amp;= \sum_{y \in Y} p(y) \sum_{x \in X} - p(x|y) \log p(x | y)\\
                       &amp;= \sum_{y \in Y} \sum_{x \in X} - p(y) p(x|y) \log p(x | y)\\
                       &amp;= \sum_{y \in Y} \sum_{x \in X} - p(y \land x) \log p(x | y)\\
    \end{aligned}\]</span></p>
<p>Kullback-Leibler divergence <span><span class="math inline">\(D(X || Y)\)</span></span>: The Kullback-Leibler divergence of <span class="math inline">\(X \sim p \)</span> with respect to <span class="math inline">\(X&#39; \sim q\)</span> is: <span class="math display">\[\begin{aligned}
        {\ensuremath{D(X || X&#39;)}} \equiv \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}
    \end{aligned}\]</span></p>
<p>Note that <span><span class="math inline">\(D(X || X&#39;)\)</span></span> is <em>not symmetric</em>.</p>
<p>Intuition: extra cost of encoding <span class="math inline">\(X\)</span> if we thought the distribution were <span class="math inline">\(X&#39;\)</span>.</p>
<p>Useful extremal case to remember: Assume <span class="math inline">\(X&#39;\)</span> has <span class="math inline">\(q(x) = 0\)</span> for some letter <span class="math inline">\(x \in X\)</span>. In this case, <span class="math inline">\({\ensuremath{D(X || X&#39;)}}\)</span> would involve a term <span class="math inline">\(\frac{p(x)}{0}\)</span>, which is <span class="math inline">\(\infty\)</span>. This is intuitively sensible, since <span class="math inline">\(X&#39;\)</span> has no way to represent <span class="math inline">\(x\)</span>, and hence <span class="math inline">\(X&#39;\)</span> is <em>infinitely far away from encoding <span class="math inline">\(X\)</span></em>. However, In this same case, one could have that <span class="math inline">\(X\)</span> is able to encode all of <span class="math inline">\(X&#39;\)</span>.</p>
<p>Mutual information: <span><span class="math inline">\(I\)</span></span>(X;Y): This is the relative entropy between the joint and product distributions.</p>
<p><span class="math display">\[\begin{aligned}
        {\ensuremath{I}}(X;Y) &amp;\equiv {\ensuremath{D(p(x, y) || p(x)p(y))}} \equiv H(X) - H(X|Y) \\
                &amp;= \sum_x \sum_y p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
    \end{aligned}\]</span></p>
<p>Proof of equivalence of two definitions of mutual information:</p>
<p><span class="math display">\[\begin{aligned}
        &amp;{\ensuremath{I}}(X;Y)  = \sum_x \sum_y p(x, y) \log \frac{p(x, y)}{p(x)p(y)} \\
        &amp;= \sum_x \sum_y p(x, y) \log \frac{p(x|y)}{p(x))} \\
        &amp;= - \sum_x \sum_y p(x, y) \log p(x) - \big[ - \sum_x \sum_y p(x, y) \log p(x|y) \big] \\
        &amp;= - \sum_x p(x) \log p(x) - \big[ - \sum_y \sum_x p(x, y) \log p(x|y) \big] \\
        &amp;= - \sum_x p(x) \log p(x) - \big[ - \sum_y p(y) \sum_x \log p(x|y) \big] \\
        &amp;= {\ensuremath{H}}(X) - H(X|Y)
    \end{aligned}\]</span></p>
<p>Some notes about mutual information:</p>
<ul>
<li><p><span class="math inline">\(I(X; Y) = I(Y;X)\)</span>. That is, <span class="math inline">\(I\)</span> is symmetric.</p></li>
<li><p>Since <span class="math inline">\(I(X;Y) = H(X) - H(X|Y)\)</span>, one can view it as the <em>reduction in uncertainity</em> of <span class="math inline">\(X\)</span>, after knowing <span class="math inline">\(Y\)</span>. Alternatively, (to avoid double negatives), it’s the <em>gain in certainty</em> of <span class="math inline">\(X\)</span> after knowing <span class="math inline">\(Y\)</span>. Another way of saying this is, what is the expected reduction in the number of yes/no questions to be answered to isolate the value of <span class="math inline">\(X\)</span> on knowing the value of <span class="math inline">\(Y\)</span>.</p></li>
<li><p><span class="math inline">\(I(X;Y) = 0\)</span> iff <span class="math inline">\(X, Y\)</span> are independent. That is, knowing <span class="math inline">\(X\)</span> reduces no uncertainity about <span class="math inline">\(Y\)</span>.</p></li>
<li><p><span class="math inline">\(I(X;X) = H(X)\)</span>. So, knowing <span class="math inline">\(X\)</span> allows us to reduce our uncertainity of <span class="math inline">\(X\)</span> by <span class="math inline">\(H(X)\)</span>. ie, we completely know <span class="math inline">\(X\)</span>, since we have <em>reduced our uncertainity of X</em> which was initially <span class="math inline">\(H(X)\)</span>, by <span class="math inline">\(H(X)\)</span>.</p></li>
</ul>
<p>Chain rule for entropy: Let <span class="math inline">\(X_1, X_2, \dots X_n \sim p(x_1, x_2, \dots, x_n)\)</span> Then: <span class="math display">\[\begin{aligned}
        {\ensuremath{H}}(X_1, X_2, \dots X_n) = \sum_i {\ensuremath{H}}(X_i | X_{i-1}, X_{i-2}, \dots X_1)
    \end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
    &amp;{\ensuremath{H}}(X_1, X_2) = {\ensuremath{H}}(X_1) + {\ensuremath{H}}(X_2 | X_1) \\
    &amp;{\ensuremath{H}}(X_1, X_2, X_3) = {\ensuremath{H}}(X_1) + {\ensuremath{H}}(X_2, X_3 | X_1) = {\ensuremath{H}}(X_1) + H(X_2 | X_1) + H(X_3 | X_2, X_1) \\
    &amp;\text{induction for the rest}
    \end{aligned}\]</span></p>
<p>Conditional mutual information: Conditional mutual information of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span> is: <span class="math display">\[\begin{aligned}
        {\ensuremath{I}}(X; Y | Z) \equiv {\ensuremath{H}}(X|Z) - {\ensuremath{H}}(X|Y, Z)
    \end{aligned}\]</span></p>
<p>Chain rule for information: <span class="math display">\[\begin{aligned}
        {\ensuremath{I}}(X_1, X_2, \dots X_n ; Z) = \sum_i I(X_i ; Z | X_1, X_2, \dots X_{i - 1}
    \end{aligned}\]</span></p>
<p>TODO: finish</p>
<h1 id="variational-auto-encoders">Variational auto encoders</h1>
<h1 id="learning-a-graphical-model">Learning a graphical model</h1>
<p>The graph of the model is fixed, we are interested in learning the parameters. This is unsupervised learning. We want to learn the distribution from which the samples were generated.</p>
<p>We will use <em>maximum likelihood</em>. There is a dataset <span class="math inline">\(D\)</span> which is sampled IID from a distribution <span class="math inline">\(p^\star\)</span>.</p>
