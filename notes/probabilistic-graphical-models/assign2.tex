% Section 3.5 (Q1 and Q2)
% Section 4.5 (Q1)
% Section 6.8 (Q1)


\documentclass[11pt]{article}
%\documentclass[10pt]{llncs}
%\usepackage{llncsdoc}
\linespread{1.025}              % Palatino leads a little more leading
\usepackage{physics}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listing}
\usepackage{minted}
\usepackage{tikz}
\evensidemargin=0.20in
\oddsidemargin=0.20in
\topmargin=0.2in
%\headheight=0.0in
%\headsep=0.0in
%\setlength{\parskip}{0mm}
%\setlength{\parindent}{4mm}
\setlength{\textwidth}{6.4in}
\setlength{\textheight}{8.5in}
%\leftmargin -2in
%\setlength{\rightmargin}{-2in}
%\usepackage{epsf}
%\usepackage{url}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{enumitem}
%\usepackage{minted}
%\newminted{fortran}{fontsize=\footnotesize}

\usepackage{xargs}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}

\usepackage{epsfig}
\usepackage{tabularx}
\usepackage{latexsym}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\E}[1]{\ensuremath{\mathbb{E} \left[ #1 \right]}}
\renewcommand{\P}[1]{\ensuremath{\mathbb{P} \left[ #1 \right]}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\R}{\ensuremath{\mathbb R}}
\newcommand{\Rgezero}{\ensuremath{\mathbb R_{\geq 0}}}
\newcommand{\powerset}{\ensuremath{\mathcal{P}}}

\newcommand{\boldX}{\ensuremath{\mathbf{X}}}
\newcommand{\boldY}{\ensuremath{\mathbf{Y}}}


\newcommand{\G}{\ensuremath{\mathcal{G}}}
\newcommand{\D}{\ensuremath{\mathcal{D}}}
\renewcommand{\H}{\ensuremath{\mathcal{H}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\DeclareMathOperator{\vcdim}{VCdim}
\newcommand{\Vcdim}{\ensuremath{\vcdim}}
\newcommand{\VCdim}{\ensuremath{\vcdim}}
\newcommand{\VCDim}{\ensuremath{\vcdim}}

\def\qed{$\Box$}
\newtheorem{question}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{proof}{Proof}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\title{Probabilistic graphical models, Assignment 2}
\author{Siddharth Bhat (20161105)}
\date{March 21st, 2020}


\begin{document}
\maketitle
\section*{3.5, Q1:}
Monotonicity of Sample Complexity: Let $\H$ be a hypothesis class for a
binary classification task. Suppose that $\H$ is PAC learnable and its sample
complexity is given by $m_\H (\cdot, \cdot)$. Show that $m_\H$ is monotonically
increasing in both parameters. That is, show that:
\begin{enumerate}
    \item for $\delta \in (0, 1)$ for $0 \leq \epsilon_1 \leq \epsilon_2 \leq 1$,
        show that $m_\H(\epsilon_1, \delta) \geq m_\H(\epsilon_2, \delta)$.
    \item  for $\epsilon \in (0, 1)$ for $0 \leq \delta_1 \leq \delta_2 \leq 1$,
        show that $m_\H(\epsilon, \delta_1) \geq m_\H(\epsilon, \delta_2)$.
\end{enumerate}
\subsection*{Solution, fixed $\delta$}
First recall what it means to be PAC-learnable:

\begin{quote}
For every $\epsilon, \delta \in (0, 1)$, for every distribution $\D$ over $\X$,
for every labelling function $f: \X \rightarrow \{0, 1\}$, if the realisability
assumption holds with respect to $(\H, \D, f)$, then for all constants
$m \geq m_\H(\epsilon, \delta)$ , on running the learning algorithm on
a random i.i.d sample $S \in X^m \sim \D^m$, the algorithm produces a hypothesis
$h_S$, such that with probability $(1 - \delta)$, the learning error 
$L_{(\D, f)}(h) \leq \epsilon$.
\end{quote}

$\epsilon_1$ and $\epsilon_2$ are the error rates that we wish to achieve.
Intuitively, the smaller the error we want, the more samples we need. 
Hence $\epsilon_1 \leq \epsilon_2 \implies m_\H(\epsilon_1, \delta) \geq m_\H(\epsilon_2, \delta)$.

Formally, we shall provide a \textbf{proof by contradiction}. Let us
assume that:
$$
\epsilon_1 \leq \epsilon_2 \implies m_\H(\epsilon_1, \delta) \leq m_\H(\epsilon_2, \delta) \qquad 
\text{(assumption for contradiction)}
$$

Unwrapping the definition, this means that:

\begin{itemize}
    \item[1] Giving the learning algorihm $m_1 = m_\H(\epsilon_1, \delta)$
        samples, the best error rate we are able to achieve is $\epsilon_1$,
        $(1- \delta)$ of the time.
    \item[2] Similarly, on giving the learning algorithm learning algorithm
        $m_2 = m_\H(\epsilon_1, \delta)$ samples, the best error rate we are able to
        achieve is $\epsilon_2$, $(1- \delta)$ of the time.
\end{itemize}

Since the fraction $(1 - \delta)$ is the same in both cases, we can focus
purely on the error rate.

Recall that $\epsilon_1 < \epsilon_2$ and $(m_2 > m_1)$ from our assumption
of contradiction.

Hence, in the $m_2$ case, we can simply \textbf{discard samples} and provide
the learning algorithm with $m_1$ samples, thereby reducing our error
rate to $\epsilon_1$. However, we had assumed that for $m_2$, $\epsilon_2$
was \textbf{the best error rate we could have achieved}.

this contradiction arose from our contradictory assumption. Hence, it
must be the case that as $\epsilon$ decreases, the quantity $m_\H(\epsilon, \cdot)$
increses. \qed.

\rule{\textwidth}{0.1pt}

\subsection*{Solution, fixed $\epsilon$}

Once again, recall what it means to be PAC-learnable:

\begin{quote}
For every $\epsilon, \delta \in (0, 1)$, for every distribution $\D$ over $\X$,
for every labelling function $f: \X \rightarrow \{0, 1\}$, if the realisability
assumption holds with respect to $(\H, \D, f)$, then for all constants
$m \geq m_\H(\epsilon, \delta)$ , on running the learning algorithm on
a random i.i.d sample $S \in X^m \sim \D^m$, the algorithm produces a hypothesis
$h_S$, such that with probability $(1 - \delta)$, the learning error 
$L_{(\D, f)}(h) \leq \epsilon$.
\end{quote}

$\delta_1$ and $\delta_2$ are the rejection rates that we wish to achieve.
Intuitively, the smaller the number of samples we wish to reject, the more
general a solution we would need to construct, thus more samples need to
be seen.
Hence $\delta_1 \leq \delta_2 \implies m_\H(\epsilon, \delta_1) \geq m_\H(\epsilon, \delta_2)$.

Formally, we once again proceed with contradiction. We assume:

$$
\delta_1 \leq \delta_2 \implies m_\H(\epsilon, \delta_1) \leq m_\H(\epsilon, \delta_2) \qquad 
\text{(assumption for contradiction)}
$$

Unwrapping the definition, this means that:
\begin{itemize}
    \item[1] Giving the learning algorihm $m_1 = m_\H(\epsilon, \delta_1)$
        samples,  $(1 - \delta)$ of the time, the learning error will be
        less than $\epsilon$.

    \item[2] Similarly, on giving the learning algorithm learning algorithm
        $m_2 = m_\H(\epsilon_1, \delta)$ samples, $(1 - \delta_2)$ of the
        time, the learning error will be less that $\epsilon$.
\end{itemize}


\rule{\textwidth}{1pt}
\section*{3.5, Q2:} 
Let $\X$ be a discrete domain, and let 
$\H_{singleton} \equiv \{ [z] : z \in \X \} \cup \{h^− \}$, where
\begin{align*}
&[z]: \X \rightarrow \{0,1\};  \quad [z](x) \equiv \begin{cases} 1 & x = z \\ 0 & \text{otherwise} \end{cases}\\
&h^−: \X \rightarrow \{0, 1\}; \quad h^-(\_) \equiv 0
\end{align*}

The realizability assumption here implies that the true hypothesis $f$ labels
negatively all examples in the domain, perhaps except one.

\begin{enumerate}
    \item Describe an algorithm that implements the ERM rule for learning
        $\H_{singleton}$ in the realizable setup.
    \item Show that $\H_{singleton}$ is PAC learnable. Prove an upper
        bound on the sample complexity.
\end{enumerate}

\subsection*{Solution, part (a)}
Let $\X$ be the domain, let $f: \X \rightarrow \{0, 1\}$ be the underlying
target function $f$ that we are trying to approximate using $\H$.

We define the sample loss $L_S(h)$ as the number of elements in $S$
that are mis-classified by $h$. More formally,
$L_S(h) \equiv |\{ (x, y) \in \S : h(x) \neq  y \}|$.

The ERM algorithm must, given a particular sample set $S \in \X^n \sim \D^n$,
provides a function $h_0 \in \H = ERM(S)$ which has minimum sample loss
$L_S(h_0)$ across all functions in $\H$.

We can check over the classification of all the samples $s \in S$.
\begin{itemize}
    \item[-] If all samples $s \in S $ are classified as $0$:
        we return $h^-$ --- this will always return $0$.
    \item[-] If some sample $s_1 \in S$ is classified as $1$: notice that
        our hypothesis space $\H$ can only allow us to set
        \emph{at most one sample to 1}. So, we can pick \emph{any} sample
        $s_1$ to create our hypothesis function $h = [s_1]$, since that
        is the best we can do.
\end{itemize}

\begin{minted}{py}
def hminus(_): return 0 # h-: sends all samples to 0
def indicator(z): return lambda x: 1 if x == z else 0 #indicator of z
def erm_sample(S):
   # all samples which have label 1
   one_samples = [y for (x, y) in S if y == 1]
   if len(one_samples) == 0: return hminus # send all samples to 0!
   else: # we will have at least on element in one_samples
       return indicator(ones_samples[0])
\end{minted}


\subsection*{Solution, part (b)}

The definition of a hypothesis class $\H$ to be PAC-learnable is that there exists
a function $m_\H: (0, 1)^2 \rightarrow \mathbb N$ and a learning algorithm
such that:

\begin{quote}
For every $\epsilon, \delta \in (0, 1)$, for every distribution $\D$ over $\X$,
for every labelling function $f: \X \rightarrow \{0, 1\}$, if the realisability
assumption holds with respect to $(\H, \D, f)$, then for all constants
$m \geq m_\H(\epsilon, \delta)$ , on running the learning algorithm on
a random i.i.d sample $S \in X^m \sim \D^m$, the algorithm produces a hypothesis
$h_S$, such that with probability $(1 - \delta)$, the learning error 
$L_{(\D, f)}(h) \leq \epsilon$.
\end{quote}

\textbf{TODO}


\rule{\textwidth}{1pt}

\section*{4.5, Q1:}
Prove that the following two statements are equivalent for any learning
algorithm $A$, any probability distribution $\D$, and any loss function
whose loss is in the range $[0, 1]$:

\begin{align*}
    &(1): \quad \forall \epsilon, \delta > 0, \exists M \equiv m(\epsilon, \delta), \forall m \geq M :
    \underset{S \sim D^m }{\mathbb P}[L_\D(A(S)) > \epsilon] < \delta. \\
    & \qquad \qquad \Updownarrow \\
    &(2): \quad \lim_{m \rightarrow \infty} \underset{S \sim \D^m }{\mathbb E}[L_\D(A(S)) > \epsilon] = 0. \\
\end{align*}


\subsection*{Solution: $(2) \implies (1)$ }
We start with $(2)$:

\begin{align*}
    (2): ~ &\lim_{m \rightarrow \infty} \underset{S \sim \D^m }{\mathbb E}[L_\D(A(S)) > \epsilon] = 0. \\
\end{align*}

We then look at the definition of the limit abstractly, for a function $f: \mathbb N \rightarrow \mathbb R$
(In our case, the function takes $m\in \mathbb N$ as input and produces $\mathbb E[\dots]$ as output):

\begin{align*}
&\lim_{m \rightarrow \infty} f(m) = y^\star \equiv \\
    &\qquad 
    \forall p > 0, ~ \exists M \in \mathbb N,
    \forall m \in \mathbb N, m \geq M \implies 
    |f(m) - y^\star| < p
\end{align*}

Plugging in to our case, we recieve:

\begin{align*}
    \forall p > 0, ~ \exists M \in \mathbb N, 
    \forall m \in \mathbb N, m \geq M \implies 
    \left|\underset{S \sim \D^m }{\mathbb E}[L_\D(A(S)) > \epsilon]  - 0\right| < p
\end{align*}

Recall that our loss function $L_\D$ is non-negative, hence we can remove
the $|\cdot|$ and $-0$ completely, giving:


\begin{align*}
    \forall p > 0, ~ \exists M \in \mathbb N, 
    \forall m \in \mathbb N, m \geq M \implies 
    \underset{S \sim \D^m }{\mathbb E}[L_\D(A(S)) > \epsilon] < p
\end{align*}

However, note that $L_\D(A(S)) > \epsilon$ is a binary random variable, and
hence:

{\footnotesize
$$
\underset{S \sim \D^m }{\mathbb E}[L_\D(A(S)) > \epsilon] = 
1 \cdot \underset{S \sim \D^m }{\mathbb P}[L_\D(A(S)) > \epsilon] +
0 \cdot \underset{S \sim \D^m }{\mathbb P}[L_\D(A(S)) \not> \epsilon]  = 
\underset{S \sim \D^m }{\mathbb P}[L_\D(A(S)) > \epsilon] 
$$
}

Plugging this in, we get:

\begin{align*}
    \forall p > 0, ~ \exists M \in \mathbb N, 
    \forall m \in \mathbb N, m \geq M \implies 
    \underset{S \sim \D^m }{\mathbb P}[L_\D(A(S)) > \epsilon]  < p
\end{align*}

If we replace $p$ with $\delta$ in the above, we recover $(1)$.

Hence, we show that $(2) \implies (1)$. \qed



\subsection*{Solution: $(1) \implies (2)$ }

To show that $(2) \implies (1)$ we only ever argued with equalities.
We can run the proof backwards to derive $(1) \implies (2)$. \qed

\rule{\textwidth}{1pt}


\section*{6.8, Q1:} For two hypothesis classes $\H, \H'$, if $\H' \subseteq \H$
then $\vcdim(H') \leq \vcdim(H)$.
\subsection*{Solution}
Recall that the VC dimension of a given set family $\H$ is the size of
the largest set $C$ such that $H$ shatters $C$. That is, the intersection of $C$
with every element is $H$ is equal to the powerset of $C$:

\begin{align*}
\Vcdim(\H) \equiv \max_{C} \{ h \cap C : h \in \H \} = 2^C \qquad \text{We denote powerset of $C$ by $2^C$ }
\end{align*}

Now, if a set family $\H'$ is a subset of another set family $\H$, and if $\H'$
shatters $C$, then:

\begin{align*}
    &\H' \text{ shatters C} \equiv  \{ h \cap C : h \in \H'\} = 2^C \qquad \text{Given, (1)}\\
    &\{ h \cap C : h \in \H' \} \subseteq \{ h \cap C : h \in \H \} \qquad \text{Since $H' \subseteq H$} \\
    &2^C \subseteq \{ h \cap C : h \in \H \}  \qquad \text{From (1)} \\
\end{align*}

Hence, any set that can be shattered by $H'$ can be shattered by $H$ if
$H' \subseteq H \implies \vcdim(H') \leq \vcdim(H)$.

On the other hand, clearly if $H$ is larger than $H'$, then $H$ can shatter more.
For example, let $H' = \emptyset \subsetneq H$. Then $H'$ can only shatter the empty
set, while $H$ can in general shatter sets larger than the empty set. Hence,
we have have strict inequality: $\vcdim(\emptyset) < \vcdim(H)$ for example.

\end{document}
