\documentclass[11pt]{book}
%\documentclass[10pt]{llncs}
%\usepackage{llncsdoc}
\usepackage[sc,osf]{mathpazo}   % With old-style figures and real smallcaps.
\linespread{1.025}              % Palatino leads a little more leading
% Euler for math and numbers
\usepackage[euler-digits,small]{eulervm}
\usepackage{physics}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listing}
\usepackage{minted}
\evensidemargin=0.20in
\oddsidemargin=0.20in
\topmargin=0.2in
%\headheight=0.0in
%\headsep=0.0in
%\setlength{\parskip}{0mm}
%\setlength{\parindent}{4mm}
\setlength{\textwidth}{6.4in}
\setlength{\textheight}{8.5in}
%\leftmargin -2in
%\setlength{\rightmargin}{-2in}
%\usepackage{epsf}
%\usepackage{url}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{enumitem}
%\usepackage{minted}
%\newminted{fortran}{fontsize=\footnotesize}

\usepackage{xargs}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}

\usepackage{epsfig}
\usepackage{tabularx}
\usepackage{latexsym}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\R}{\ensuremath{\mathbb R}}
\renewcommand{\P}[1]{\ensuremath{\mathbb{P} \left[ #1 \right] }}
\newcommand{\coT}{\ensuremath{T^*}}
\newcommand{\Lie}{\ensuremath{\mathfrak{L}}}
\newcommand{\pushforward}[1]{\ensuremath{{#1}_{\star}}}
\newcommand{\pullback}[1]{\ensuremath{{#1}^{\star}}}

\newcommand{\pushfwd}[1]{\pushforward{#1}}
\newcommand{\pf}[1]{\pushfwd{#1}}

\newcommand{\boldX}{\ensuremath{\mathbf{X}}}
\newcommand{\boldY}{\ensuremath{\mathbf{Y}}}

\DeclareMathOperator*{\argmin}{argmin} % no space, limits underneath in displays
\DeclareMathOperator*{\argmax}{argmax} % no space, limits underneath in displays


\newcommand{\G}{\ensuremath{\mathcal{G}}}
% \newcommand{\braket}[2]{\ensuremath{\left\langle #1 \vert #2 \right\rangle}}


\def\qed{$\Box$}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}[corollary]{Theorem}
\newtheorem{definition}[corollary]{Definition}
\newtheorem{lemma}[corollary]{Lemma}
\newtheorem{observation}[corollary]{Observation}
\newtheorem{proof}[corollary]{Proof}
\newtheorem{remark}[corollary]{Remark}
\newtheorem{example}[corollary]{Example}

\title{Game theory}
\author{Siddharth Bhat}
\date{Spring 2020}

% References: 
% https://www.sv.uio.no/econ/personer/vit/bardh/dokumenter/econ5200-notes-harstad.pdf

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}


TODO: find out and write about: 
Extrinsic form representation of a game
$\Gamma \equiv \langle N, T, Z, o, A, s, u_i, \mathcal{H}\rangle$
where $N$ is number of players, $T$ is game tree, $Z$ is leaves, $o$ is owner
function, $A$ is actions, $s$ is transition, $u_i$ is player function, $\mathcal H$
is information sets. An information set contains equivalence classes
of states that the player cannot distinguish between. There can be ambiguity
due to missing information. Perfect information game is one where all
information sets are singleton. For example, chess is perfect information.
Example of partial information is card games.

Next, we look at strategies. A strategy is a computable function by which
each player selects their actions.

Game tree for matching coins with observation:
$S_A: \{ 1 \} \rightarrow \{H, T\}$. $S_B: \{2, 3\} \rightarrow \{H, T \}$.


Game tree for matching coins without observation:
$S_A: \{ 1 \} \rightarrow \{H, T\}$. $S_B: \{\{2, 3\}\} \rightarrow \{H, T \}$.

We let $S \equiv S_1 \times S_2 \times \dots \times S_n$. This a set containing
all possible strategies, called as a "strategy profile", $s \in S$. We
write $s$ as $s \equiv (s_i, s_{-i})$, where $s_{-i}$ is cute notation for
"the rest of the players". For example, if $s \equiv (s_1, s_2, s_3)$ we can 
notate $s = (s_2, s_{-2}) = (s_2, s_1, s_3)$.

We will currently focus on pure strategies, where we have a deterministic
function per strategy.


\section{Normal form games}
Another representation for games is called as strategic form / matrix form / 
normal form games.  Here, a game $\Gamma \equiv \langle N, (S_i)_{i \in N}, (u_i: S \rightarrow \R)_{i \in N} \rangle$.
$N$ is the number of players, $S_i$ are strategies for each player, $u_i$ 
are the utility functions / payoffs for each player. $u_i$ maps each
strategy profile $s$ how worth it it is for player $i$ if
the game proceeds with strategy profile $s$.

\subsection{Normal form for matching coins with observation}
\begin{align*}
   &S_a \equiv \{ S_a^1 (H), S_a^2 (T)\} \\
   &S_b \equiv \{ S_b^1 (HH), S_b^2 (HT), S_b^3 (TH), S_b^4 (TT) \} \\
   &u_a: S \rightarrow \R \\
   &u_a((s_a^1, s_b^1)) = +10 \qquad (H, HH) \\
   &u_a((s_a^1, s_b^2)) = +10 \qquad (H, HT) \\
   &u_a((s_a^1, s_b^3)) = -10 \qquad (H, TH) \\
   &u_a((s_a^1, s_b^4)) = -10 \qquad (H, TT) \\ 
   &u_a((s_a^2, s_b^1)) = -10 \qquad(T, HH) \\
   &u_a((s_a^2, s_b^2)) = +10 \qquad(T, HT)\\
   &u_a((s_a^2, s_b^3)) = -10 \qquad(T, TH) \\
   &u_a((s_a^2, s_b^4)) = +10 \qquad(T, TT) \\
\end{align*}

Alternate representation of same game:
$$
\begin{tabular}{ccccc}
    $s_b \rightarrow$ & HH & HT & TH & TT \\ 
    H & (10, -10) & (10, -10)  & (-10, +10)    & (-10, +10)   \\
    T &  (-10, 10)   &  (10, -10)  & (-10, 10)   &  (10, -10)  \\
\end{tabular}
$$


\subsection{Normal form for matching coins without observation}

$$
\begin{tabular}{ccccc}
    $s_a \rightarrow$  & H & T & \\
    H & (10, -10) & (-10, 10) \\ 
    T &  (-10, 10) & (10, -10)
\end{tabular}
$$


\subsection{Normal form for prisoners dilemma}

$C$ for confess, $NC$ for not confess.

$$
\begin{tabular}{ccccc}
     & C & NC & \\
    C & (-5, -5) & (-1, -10) \\ 
    NC &  (-10, -1) & (-2, -2)
\end{tabular}
$$

\chapter{Game analysis}
Rationality implies that each player is motivated to maximise his own payoff.
Intelligent implies that player can take into account all available
information. An intelligent and rational player implies that every player will
attempt to maximise their utility.

\subsection{Common knowledge and puzzles about common knowledge}
\begin{definition}
    \emph{Common knowledge} --- Player knows it. Every player knows that every player
    knows it. Every player knows that every player knows that
    every player knows it.
    $\forall k \in \N, (\text{Every player knows that})^k$ every player knows it.
\end{definition}

If we have an island with two water streams and all humans and intelligent,
rational and cannot speak. They have a rule that says that if a person has a
blue mark on their forehead, the drink water from a stream farther away. One
day, a visitor, who knows the above fact, shouts "why is a person with a blue
mark drinking water here?" The next day, no one comes to the stream. What changed?
The only difference before and after is that it is now common knowledge that
there is one person with a blue mark drinking water at the stream. This


Some one imagined two positive whole numbers $1 \leq a, b \leq 20$. He tells the sum of these
two numbers to mathematician $A$, the product of these numbers to mathematician $B$.
$A$ tells $B$ that there is no way for $B$ to know the sum. Then $B$ exclaims
"But I know the sum now!", to which $A$ exclaims "and now I know the product".

\subsection{Strongly dominated strategy}

Given a game $\Gamma \equiv \langle N, (S_i), (u_i) \rangle$, a strategy
$s_i \in S_i$ is said to be strongly dominated by a strategy $s_i' \in S_i$
if:

$$
u_i(s_i, s_{-1}) < u_i(s_i', s_{-i})~ \forall s_{-i} \in S_{-i}
$$

\subsection{Strongly dominant strategy}

Given a game $\Gamma \equiv \langle N, (S_i), (u_i) \rangle$, a strategy
$s_i^\star$ is said to be strongly dominant if it strongly dominates every
other strategy $s_i \in S_i$.

$$
\forall s_i \in S_i, s_i \neq s_i^\star \implies ~ u_i(s_i, s_{-1}) < u_i(s_i^\star, s_{-i})~ \forall s_{-i} \in S_{-i}, 
$$

Note that to analyze strongly dominated and strongly dominant strategies,
we only need $u_i$, the utility of the $i$th player. Hence, to analyze
dominance of strategies, we can get away with writing the utility of
just a single player.

$$
\begin{tabular}{ccc}
    & L & R \\
  A & 6, - & 7, - \\
  B & 5, - & 6, - \\
  C & 4, - & 6, - \\
\end{tabular}
$$

$L, R$ are moves of the player.
$A, B, C$ are stratgies with utilities filled
in.

$A$ strongly dominate $C$, $A$ strongly dominates $B$. $B$ does \emph{not}
strongly dominate $C$, since on the $R$ action, we have $6$ for both $B$ and $C$.
Hence, $A$ is the strongly dominant strategy. Note that there need not always
exist a strongly dominant strategy:

$$
\begin{tabular}{ccc}
    & L & R \\
  A & 6, - & 7, - \\
  B & 7, - & 6, - \\
\end{tabular}
$$

Neither $A$ nor $B$ are strictly better than the other.

\subsection{Weakly dominated strategy}

Given a game $\Gamma \equiv \langle N, (S_i), (u_i) \rangle$, a strategy
$s_i \in S_i$ is said to be weakly dominated by a strategy $s_i' \in S_i$
if:

$$
u_i(s_i, s_{-1}) \leq u_i(s_i', s_{-i})~ \forall s_{-i} \in S_{-i}
$$

with strict inequality for at least one $s_{-i}$. 


\subsection{Weakly dominant strategy}

Given a game $\Gamma \equiv \langle N, (S_i), (u_i) \rangle$, a strategy
$s_i^\star$ is said to be weakly dominant if it weakly dominates every
other strategy $s_i \in S_i$.

$$
\forall s_i \in S_i, s_i \neq s_i^\star \implies ~ u_i(s_i, s_{-1}) \leq u_i(s_i^\star, s_{-i})~ \forall s_{-i} \in S_{-i}, 
$$

with strict inequality for at least one $s_{-i}$. 

Once again, a weakly dominant strategy need not always exist:
$$
\begin{tabular}{ccc}
    & L & R \\
  A & 6, - & 7, - \\
  B & 7, - & 6, - \\
\end{tabular}
$$

\subsection{Very weakly dominated strategy}

Given a game $\Gamma \equiv \langle N, (S_i), (u_i) \rangle$, a strategy
$s_i \in S_i$ is said to be very weakly dominated by a strategy $s_i' \in S_i$
if:

$$
u_i(s_i, s_{-1}) \leq u_i(s_i', s_{-i})~ \forall s_{-i} \in S_{-i}
$$

Note that we do not have the strict inequality requirement anymore. This
is now a true partial order.

\subsection{Example of strong dominance in prisoners dilemma}
$$
\begin{tabular}{ccccc}
     & C & NC & \\
    C & (-5, -5) & (-1, -10) \\ 
    NC &  (-10, -1) & (-2, -2)
\end{tabular}
$$
Here, $C$ is the strongly dominant strategy for both players.


\subsection{Another game}
$$
\begin{tabular}{ccccc}
      & a & b & c \\
    A & 5 & 5 & 5 \\
    B & 4 & 5 & 5\\
    C & 4 & 4 & 4
\end{tabular}
$$

There does not exist a strongly dominant strategy.
$A$ is weakly dominant. $C$ is weakly dominated by both $A, B$.

\subsection{Can there exist two weakly dominant strategies?}
No there cannot. If $A$ is a weakly dominant strategies, then
assume $A[i] > B[i]$.If $A[i] > B[i]$, then $B$ cannot weakly dominate 
$A$, since for $B$ to dominate $A$ we need $B[j] \geq A[j]~ \forall j$,
but $B[i] < A[i]$.


\subsection{Strongly (Weakly) Dominant Strategy Equilibrium}
A strategy profile $(s_1^\star, s_2^\star, \dots, s_n^\star)$ is called as a strongly
dominant strategy equilibrium of the game $\Gamma \equiv \langle N, (S_i), (U_i) \rangle$ iff
the strategy $s_i^\star$ is a strongly dominating strategy for player $i$.

Recall the example of Prisoners dilemma ---

$$
\begin{tabular}{ccccc}
     & C & NC & \\
    C & (-5, -5) & (-1, -10) \\ 
    NC &  (-10, -1) & (-2, -2)
\end{tabular}
$$

\chapter{Lecture 2}
Missed! write this down. TODO!

\section{Review of lecture 2}

Two player zero sum games. $N = \{1, 2\}$. $u_1 = - u_2$. Also called
matrix games since the game can be represented as a matrix. $a_{ij}$ is
the utility for row player playing row $i$, column player is playing
column $j$.

We showed that we are not guaranteed a dominant strategy. Hence, we need
to look at other types of equilibria.

Here, we define saddle points. 

\chapter{Equilibrium in two-player zero-sum games}
\section{Saddle points}
Given a matrix $A$, $(i^*, j^*)$ is a saddle point iff:
\begin{align*}
    &a_{i^\star j^\star } \leq a_{i^\star K} \qquad \forall K = 1, 2, \dots, n \\
    &a_{i^\star j^\star } \geq a_{L j^\star } \qquad \forall L = 1, 2, \dots, m \\
\end{align*}
That is, $a_{i^\star j^\star }$ is the maximum in the column $j^\star$, minimum in the column
$i^\star $.


Let's now analyze the setting and try to deduce when a saddle point
exists from the entries of the matrix. For this, we define:
\begin{align*}
&u_R \equiv \max_i \min_j a_{ij} \\
&u_c \equiv \min_j \max_i a_{ij}
\end{align*}

\begin{lemma}
    If there are multiple saddle points, $(i_1, j_1)$ and $(i_2, j_2)$, then
    $a_{i_1 j_1} = a_{i_2 j_2}$
\end{lemma}
\begin{proof} 
    Let $a[c][d], a[p][q]$ be two saddle points. By definition, a saddle
    point is greater than everything else on the same row, and less than
    everything else on the same column.

    \begin{align*}
        \begin{bmatrix}
        (c, d)   & > & (c, q) \\
        \wedge & & \vee \\
        (p, d) & < & (p, q)
        \end{bmatrix}
    \end{align*}

    We see that $(c, d) > (c, q) > (p, q)$, and $(c, d) < (p, d) < (p, q)$,
    which implies $(c, d) = (p, q)$.
\end{proof}

\begin{lemma}
    $u_r \leq u_c$
\end{lemma}
\begin{proof} TODO \end{proof}

\begin{lemma}
    $u_r = u_c \implies $~saddle point exists for $A$.
\end{lemma}
\begin{proof} 
Consider $u_R \equiv \max_i \min_j a[i][j]$. Let us introduce a symbol
$minR(i) \equiv min_j a[i][j]$ Hence, $u_R = max_i minR(i)$. This shows us
that $u_R$ is the minimum value in its row.

Similarly, we consider $u_C \equiv \max_i \min_j a[i][j]$. As before, introduce
$maxC(j) \equiv max_i a[i][j]$ Hence, $u_C = min_j maxC(j)$.Hence, $u_C$ is 
the maximum value in its column (Notice the duality). 

Let $(c, d)$ be the index of $u_R$, and $(p, q)$ be the index of $u_C$.

Now consider the matrix:

\begin{align*}
    \begin{bmatrix}
    (c, d)   & < & (c, q) \\
     & & \wedge \\
     & & (p, q)
    \end{bmatrix}
\end{align*}

Hence, $(c, d) \leq (c, q)$ (Since $u_R = a[c][d]$ is minimum in its row).
$(c, q) \leq(p, q)$ (Since $u_C = a[p][q]$ is maximum in its column).  
Therefore, $u_R = (c, d) \leq (c, q) \leq (p, q) = u_C$.
\end{proof}

\begin{lemma}
    $u_r = u_c \iff $~saddle point exists for $A$.
\end{lemma}
\begin{proof} TODO \end{proof}

\subsection{Analysis of saddle points}
If a saddle points exists, then the row player is maximising her minimum
assured gain. The column player is minimising her worst loss. 

If such a saddle point exists, it is called as an equilibrium. The strategy 
that achieves this is the \textbf{mini-max} strategy.

\subsubsection{Single saddle point}

\begin{tabular}{cccc}
    \footnotesize{A $\downarrow$ B $\rightarrow$} & L & M & R \\
    T & 1 & 2 & 1 \\
    M & 0 & -1 & 2 \\
    B & -1 & 0 & -2 \\
\end{tabular}
$(u_r \equiv 2, u_c \equiv 2)$.

\subsubsection{Multiple saddle points}

\begin{tabular}{cccc}
    & L & M & R \\
    T & 0 & 2 & 0 \\
    M & -1 & 5 & 2 \\
    B & 0 & 5 & 0 \\
\end{tabular}
There are 4 saddle points: $(T, L), (T, R), (B, L), (B, R)$.


\subsubsection{Matching coins --- no saddle point}

\begin{tabular}{ccc}
      & H & T \\
    H & 10 & -10 \\
    T & -10 & 10
\end{tabular}
$(u_R \equiv -10, u_c \equiv 10)$ --- no saddle point!

Here, notice that no deterministic strategy can win. What we should do is to
use a randomized strategy where we play $H$ or $T$ with equal probability.

\section{Mixed strategies}

As the matching coins example shows, we need to use a randomized strategy
so we are not exploited. What we can do is to maximise expected utility:

The expected utility of the row player is going to be $\P{H, H} - \P{H, T} - \P{T, H} + \P{T, T}$.

The mixed strategy space consists of probability distributions over pure strategies.
$$\Delta(S) \equiv \left\{ (p_1, p_2, \dots p_k) \mid |S| = k \quad p_i \geq 0 \quad \sum_i p_i = 1\right\}$$

We denote mixed strategies with $\sigma_i \in \Delta(S_i)$, and mixed
strategy profiles as $\sigma \equiv  (\sigma_1, \sigma_2, \dots \sigma_n) \in \prod_i \Delta(S_i)$


We calculate the expected utility as:
\begin{align*}
    U_i(\sigma_i, \sigma_{-i}) \equiv
    \sum_{s_{-1} \in S_{-i}} 
p_{i_1} p(s_{-i}) u(a_{i_1}, s_{-i}) +
p_{i_2} p(s_{-i}) u(a_{i_2}, s_{-i}) + \dots +
    p_{i_k} p(s_{-i}) u(a_{i_k}, s_{-i})
\end{align*}

For two-player zero-sum games, we refer to mixed strategies as 
$p \equiv (p_1, \dots, p_m)$,
$q \equiv (q_1, \dots q_n)^T$. This leads us to \textbf{Utility theory}.


\section{Axiomatic description of utility theory}
Let $X$ be the set of outcomes. $\geq$ (TODO: curly, need to find this) be the
preference of the player over the set of outcomes.
We define $\left( x_1 \sim x_2 \equiv x_1 \geq x_2 \land x_2 \geq x_1 \right)$.

TODO: find formal definition of a lottery.

\begin{itemize}
    \item Completeness: every pair of outcomes is ranked.
    \item Transitivity: $x_1 \geq x_2 \land x_2 \geq x_3 \implies x_1 \geq x_3$.
    \item Substitutability: if $x_1 \sim x_2$, then any lottery in which $x_1$
        is substituted by $x_2$ is equally preferred.
    \item Decomposability: If two lotteries assign the same probability to
        each outcome, then the player is indifferent between these two lotteries.
    \item Monotonicity: If $x_1 > x_2$ and $p > q$, then $[x_1: p, x_2 : 1- p] \geq [x_1 : q, x_2 : 1 - q ]$.
    \item Continuity: If $x_1 > x_2 > x_3$, then there exists $p \in [0, 1]$
        such that $x_2 \sim [x_1: p, x_3: 1 - p]$.
\end{itemize}

\begin{theorem}
    Von Neumann and Morgenstern: Given a set of outcomes $X$ has a preference
    relation on $X$ that satisfies the above axioms, there exists a utility function
    $u: X \rightarrow [0, 1]$ such that:
    \begin{itemize}
        \item $u(x_1) \geq u(x_2) \iff x_1 \geq x_2$
        \item $U([x_1: p_1, x_2:p_2, \dots x_m:p_m]) = \sum_{j=1}^m p_j u(x_j)$
    \end{itemize}
\end{theorem}



\section{Best response function}
Let $\Gamma \equiv (N, S_{i \in N}, u: \prod_i S_i \rightarrow R)$ be a normal
for game. We define the best response function for player $i$ as:
\begin{align*}
   r_i: S_{-i} \rightarrow S_i
    \qquad r_i(s_{-i}) \equiv \argmax_{s_i \in S_i}  u(s_i, s_{-i})
\end{align*}

That is, under a fixed strategy for all of $i$'s opponents, the best response
function $r_i(s_{-i})$ tells us the best move we can make.


\section{Nash equilibrium}

A strategy profile $s \equiv (s_1, s_2, \dots s_n)$ is a Nash equilibrium
if $s_i$ is the best response to $s_{-i}$ for all $i$.


\section{Writing down the Nash Equilibirum in terms of an LP}
Let $K_i$ be the set of actions that player $i$ will take (the support).
Let $p_i[s]$ be the probability for player $i$ to take action $s \in S_i$.

We need the conditions:
\begin{itemize}
    \item $\forall s \not \in K_i, p_i[s] = 0$
    \item $\sum p_i s = w_i$
\end{itemize}


\section{Kakutani fixed point theorem}
consider a relation $R: \Sigma \rightrightarrows \Sigma$ such that:
\begin{itemize}
    \item $\Sigma$ is compact, convex, and nonempty.
    \item $R(\sigma)$ is nonempty.
    \item $R(\sigma)$ is convex.
    \item $R$ has a closed graph.
\end{itemize}

under these conditions, $R$ is guaranteed to have a fixed point.

\section{Existence of Nash equilibrium}
We prove this using the Kakutani fixed point theorem.


\section{Existence of Nash equilibrium (Insightful version)}
We prove this using the Brouwer fixed-point theorem. While this requires
us to roll up our sleeves a little bit, the fixpoint is far more constructive,
in the sense that it tells us what the fixpoint is actually, well, fixing.

consider the correspondence
\begin{align*}
    R: Sigma \rightarrow 2^{Sigma} \qquad R(\sigma) \equiv [r_i(\sigma_{-i})]_{i \in N}
\end{align*}

That is, for each strategy profile $\sigma$, we relate it to the set
of strategy profiles that are best responses against this profile.

Now note that a nash equilibrium is a fixed point of this relation.
We will show that the above relation $R$ satisfies the assumptions
of the Kakatuni fixed point theorem, and is hence a fixed point.


\section{LP formulation for 2 player zero sum game}
\section{Lemke Howson theorem for computing mixed-strategy nash eqm}
\section{Mechanism design}
We consider a setting:

\begin{align*}
    &N \equiv \text{Set of players} \\
    &X  \equiv \text{Alternatives} \\
    &\Theta_{i \in N} \equiv \text{private information of player $i$} \\
    &\phi: \prod_i \Theta_i \rightarrow \mathbb R \equiv \text{prior probability over private information} \\
    &u_{i \in N}: \prod_i \Theta_i \times X \rightarrow \mathbb R \equiv \text{utility of player $i$} \\
\end{align*}

A mechanism is a tuple:
\begin{align*}
    &\Gamma \equiv (S_{i \in N}, g) \\
    &S_{i \in N} \equiv \text{strategies of each player $i$} \\
    &g: \prod_i S_i \rightarrow X \equiv \text{an outcomes function}
\end{align*}
A mechanism tells us what actions a player can take, and what the outcome
of each action is.

We often want the mechanism to be designed to optimised a particular social good:
\begin{align*}
    f: \prod_i \Theta_i \rightarrow X \equiv \text{Social choice function} \\
\end{align*}

That is, given knowledge of what players \emph{truly want}, we know what
outcome we are interested in. In this setting, we are then to design 
a mechanism $(S_i, g)$ which will encourage a situation where rational players
will take action dictated by $f$.

Sometimes, we will denote $S_i$ by $\hat \Theta_i$, since any action the
player can take can be mathematically modeled as a deterministic function
of their state. Hence, if we allow players to lie and publish 
a $\hat \Theta_i$, which is their reporting of their private $\Theta_i$, it's
equivalent to a setting where players have arbitrary $S_i$.

\end{document}
